{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d362591e-9f54-46f9-b703-49062ac3072f",
   "metadata": {},
   "source": [
    "# Knowledge Graph RAG\n",
    "\n",
    "<img src=\"./media/graph_start.png\" width=600>\n",
    "\n",
    "*[Improving Knowledge Graph Completion with Generative LM and neighbors](https://deeppavlov.ai/research/tpost/bn15u1y4v1-improving-knowledge-graph-completion-wit)*\n",
    "\n",
    "In the evolving landscape of AI and information retrieval, knowledge graphs have emerged as a powerful way to represent complex, interconnected information. A knowledge graph is a knowledge base that uses a graph-structured data model or topology to represent and operate on data. Knowledge graphs are often used to store interlinked descriptions of entities – objects, events, situations or abstract concepts – while also encoding the free-form semantics or relationships underlying these entities. [Source: Wikipedia](https://en.wikipedia.org/wiki/Knowledge_graph)\n",
    "\n",
    "What makes knowledge graphs particularly powerful is their ability to mirror human cognition in data. They more explicitly map the relationships between objects, concepts, or ideas together through both their semantic and relational connections. This approach closely parallels how our brains naturally understand and internalize information – not as isolated facts, but as a web of interconnected concepts and relationships.\n",
    "\n",
    "<img src=\"./media/coffee_graph_ex.png\" width=400>\n",
    "\n",
    "Looking at a concept like \"coffee,\" we don't just know it's a beverage; we automatically connect it to related concepts like beans, brewing methods, caffeine, morning routines, and social interactions. Knowledge graphs capture these natural associations in a structured way.\n",
    "\n",
    "Traditional RAG systems, while effective at semantic similarity-based retrieval, often struggle to capture broader conceptual relationships across text chunks. Knowledge Graph RAG addresses this limitation by introducing a structured, hierarchical approach to information organization and retrieval. By representing data in a graph format, these systems can traverse relationships between concepts, enabling more sophisticated query understanding and response generation. This approach allows for targeted querying along specific relationship paths, handles complex multi-hop questions, and provides clearer reasoning through explicit connection paths. The result is a more nuanced and interpretable system that combines the structured reasoning of knowledge graphs with the natural language capabilities of large language models.\n",
    "\n",
    "While [knowledge graphs are not a new concept](https://blog.google/products/search/introducing-knowledge-graph-things-not/), their creation has traditionally been a resource-intensive process. Early knowledge graphs were built either through manual curation by domain experts or by converting existing structured data from relational databases. This limited both their scale and adaptability to new domains.\n",
    "\n",
    "<img src=\"./media/table_comp.png\" width=600>\n",
    "\n",
    "*[What is a Knowledge Graph (KG)?](https://zilliz.com/learn/what-is-knowledge-graph)*\n",
    "\n",
    "The introduction of LLMs has transformed this landscape. LLMs' capabilities in NLP, reasoning, and relationship extraction now enable automated construction of knowledge graphs from unstructured text. These models can identify entities, infer relationships, and structure information in ways that previously required extensive manual labor. As a plus, this allows knowledge graphs to be dynamically updated and expanded as new information becomes available, making them more practical and scalable for real-world applications.\n",
    "\n",
    "To see this in action ourselves, and compare it to traditional vector similarity techniques, we'll take a look at Microsoft's Open Source [GraphRAG](https://microsoft.github.io/graphrag/) and how it works behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb752dad-d6bf-436f-a175-03a1d491bb3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Main Components of Knowledge Graphs\n",
    "\n",
    "**Entity**\n",
    "\n",
    "<img src=\"./media/entities.png\" width=500>\n",
    "\n",
    "An Entity is a distinct object, person, place, event, or concept that has been extracted from a chunk of text through LLM analysis. Entities form the nodes of the knowledge graph. During the creation of the knowledge graph, when duplicate entities are found they are merged while preserving their various descriptions, creating a comprehensive representation of each unique entity.\n",
    "\n",
    "**Relationship**\n",
    "\n",
    "<img src=\"./media/relationship.png\" width=400>\n",
    "\n",
    "A Relationship defines a connection between two entities in the knowledge graph. These connections are extracted directly from text units through LLM analysis, alongside entities. Each relationship includes a source entity, target entity, and descriptive information about their connection. When duplicate relationships are found between the same entities, they are merged by combining their descriptions to create a more complete understanding of the connection.\n",
    "\n",
    "**Community**\n",
    "\n",
    "<img src=\"./media/communities.png\" width=400>\n",
    "\n",
    "A Community is a cluster of related entities and relationships identified through hierarchical community detection, generally using the [Leiden Algorithm](https://en.wikipedia.org/wiki/Leiden_algorithm). Communities create a structured way to understand different levels of granularity within the knowledge graph, from broad overviews at the top level to detailed local clusters at lower levels. This hierarchical structure helps in organizing and navigating complex knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7e77f-a543-49e2-813e-c1305f7a058d",
   "metadata": {},
   "source": [
    "---\n",
    "## GraphRAG Creation Data Flow\n",
    "\n",
    "<img src=./media/graph_building.png width=1000>\n",
    "\n",
    "Indexxing in GraphRAG is an extensive process, where we load the document, split it into chunks, create sub graphs at a chunk level, combine these subgraphs into our final graph, algorithmically identify communities, then document the communities main features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19896a38-bcf7-4664-9f57-cb12cf15cdea",
   "metadata": {},
   "source": [
    "### **Loading and Splitting Our Text**\n",
    "\n",
    "For our example, we'll be using [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/pdf/2408.13296).\n",
    "\n",
    "This will be loaded as a text file (remove index, glossary, and references) and split into 1200 token, 100 token overlap chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e6a581-74a1-455e-8634-2c94d6496e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "with open(\"./paper/input/2408.13296v3.txt\", 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1200, chunk_overlap=100)\n",
    "\n",
    "texts = text_splitter.split_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dd7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 46 documents.\n",
      "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities\n",
      "\n",
      "(Version 1.1)\n",
      "\n",
      "Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and Arsalan Shahid\n",
      "\n",
      "@ CeADAR Connect Group\n",
      "\n",
      "CeADAR: Ireland's Centre for AI, University College Dublin, Belfield, Dublin, Ireland { venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie\n",
      "\n",
      "October 2024\n",
      "\n",
      "Abstract\n",
      "\n",
      "This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs), integrating theoretical insights and practical applications. It begins by tracing the historical development of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their respective implications for specific tasks.\n",
      "\n",
      "A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle from data preparation to model deployment. Key considerations include data collection strategies, handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with optimal model performance.\n",
      "\n",
      "The exploration extends to advanced fine-tuning techniques and configurations like memory finetuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods harness specialised networks and multi-agent collaboration for improved outcomes. Proximal Policy Optimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches to aligning models with human preferences, while the benefits of pruning and routing optimisations are examined for enhancing efficiency.\n",
      "\n",
      "In the latter sections, the report delves into validation frameworks, post-deployment monitoring, and optimisation techniques for inference. It also addresses the deployment of LLMs on distributed and cloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for audio and speech processing are covered, alongside emerging challenges related to scalability, privacy, and accountability.\n",
      "\n",
      "This report aims to serve as a comprehensive guide for researchers and practitioners, offering actionable insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this rapidly evolving field.\n",
      "\n",
      "Contents\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "Introduction\n",
      "\n",
      "1.1 Background of Large Language Models (LLMs)\n",
      "\n",
      "Large Language Models (LLMs) represent a significant leap in computational systems capable of understanding and generating human language. Building on traditional language models (LMs) like N-gram models [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex linguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range dependencies. Key advancements include in-context learning for generating coherent text from prompts and Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human responses. Techniques like prompt engineering, question-answering, and conversational interactions have significantly advanced the field of natural language processing (NLP) [4].\n",
      "\n",
      "1.2 Historical Development and Key Milestones\n",
      "\n",
      "Language models are fundamental to natural language processing (NLP), leveraging mathematical techniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over several decades, language modelling has evolved from early statistical language models (SLMs) to today's advanced large language models (LLMs). This rapid advancement has enabled LLMs to process, comprehend, and generate text at a level comparable to human capabilities [5, 6].\n",
      "\n",
      "Figure 1.1 shows the evolution of large language models from early statistical approaches to current advanced models.\n",
      "\n",
      "1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs\n",
      "\n",
      "Understanding LLMs requires tracing the development of language models through stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs), and LLMs.\n",
      "\n",
      "1.3.1 Statistical Language Models (SLMs)\n",
      "\n",
      "Emerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the likelihood of sentences within texts. For instance, the probability P S ( ) of the sentence 'I am very happy' is given by:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "This probability can be calculated using conditional probabilities:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Conditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\n",
      "\n",
      "<!-- missing-text -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "1.3.2 Neural Language Models (NLMs)\n",
      "\n",
      "NLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors enable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector space where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons organised into layers, resembling the human brain's structure. The input layer concatenates word vectors, the hidden layer applies a non-linear activation function, and the output layer predicts subsequent words using the Softmax function to transform\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split into {len(texts)} documents.\")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9e65e-531c-4e94-9808-4ede865621d7",
   "metadata": {},
   "source": [
    "**Entity and Relationship Extraction Prompt**\n",
    "\n",
    "This is a [tuned](https://microsoft.github.io/graphrag/prompt_tuning/auto_prompt_tuning/) entity extraction prompt used in our real GraphRAG implementation, extracted in this format to see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e329774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6236c47-8584-41d6-8553-f516e282d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "-Goal-\n",
    "Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: One of the following types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\"{{tuple_delimiter}}<entity_name>{{tuple_delimiter}}<entity_type>{{tuple_delimiter}}<entity_description>)\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\n",
    "Format each relationship as (\"relationship\"{{tuple_delimiter}}<source_entity>{{tuple_delimiter}}<target_entity>{{tuple_delimiter}}<relationship_description>{{tuple_delimiter}}<relationship_strength>)\n",
    "\n",
    "3. Return output in The primary language of the provided text is \"English.\" as a single list of all the entities and relationships identified in steps 1 and 2. Use **{{record_delimiter}}** as the list delimiter.\n",
    "\n",
    "4. If you have to translate into The primary language of the provided text is \"English.\", just translate the descriptions, nothing else!\n",
    "\n",
    "5. When finished, output {{completion_delimiter}}.\n",
    "\n",
    "-Examples-\n",
    "######################\n",
    "\n",
    "Example 1:\n",
    "\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text:\n",
    " LLMs to create synthetic samples that mimic clients’ private data distribution using\n",
    "differential privacy. This approach significantly boosts SLMs’ performance by approximately 5% while\n",
    "maintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\n",
    "solely on local private data.\n",
    "In healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\n",
    "without transferring sensitive information. This approach ensures data privacy while enabling the de-\n",
    "velopment of robust, generalisable AI systems.\n",
    "8https://ai.meta.com/responsible-ai/\n",
    "9https://huggingface.co/docs/hub/en/model-cards\n",
    "10https://www.tensorflow.org/responsible_ai/privacy/guide\n",
    "101 Frameworks for Enhancing Security\n",
    "Adversarial training and robust security measures[111] are essential for protecting fine-tuned models\n",
    "against attacks. The adversarial training approach involves training models with adversarial examples\n",
    "to improve their resilience against malicious inputs. Microsoft Azure’s\n",
    "------------------------\n",
    "output:\n",
    "(\"entity\"{{tuple_delimiter}}DIFFERENTIAL PRIVACY{{tuple_delimiter}}differential privacy{{tuple_delimiter}}Differential privacy is a technique used to create synthetic samples that mimic clients' private data distribution while maintaining data privacy with a minimal privacy budget{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}HEALTHCARE{{tuple_delimiter}}healthcare{{tuple_delimiter}}In healthcare, federated fine-tuning allows hospitals to collaboratively train models on patient data without transferring sensitive information, ensuring data privacy{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}federated learning{{tuple_delimiter}}Federated learning is a method that enables collaborative model training on decentralized data sources, such as hospitals, without sharing sensitive information{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}ADVERSARIAL TRAINING{{tuple_delimiter}}adversarial training{{tuple_delimiter}}Adversarial training involves training models with adversarial examples to improve their resilience against malicious inputs{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}SECURITY MEASURES{{tuple_delimiter}}security measures{{tuple_delimiter}}Robust security measures are essential for protecting fine-tuned models against attacks{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}DIFFERENTIAL PRIVACY{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}Differential privacy is used in federated learning to maintain data privacy while training models collaboratively{{tuple_delimiter}}8{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}HEALTHCARE{{tuple_delimiter}}FEDERATED LEARNING{{tuple_delimiter}}Federated learning is applied in healthcare to train models on patient data without transferring sensitive information{{tuple_delimiter}}9{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ADVERSARIAL TRAINING{{tuple_delimiter}}SECURITY MEASURES{{tuple_delimiter}}Adversarial training is a security measure used to protect models against attacks by improving their resilience{{tuple_delimiter}}8{{completion_delimiter}}\n",
    "#############################\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text:\n",
    "ARD [82] is an innovative open-source tool developed to enhance the safety of interactions\n",
    "with large language models (LLMs). This tool addresses three critical moderation tasks: detecting\n",
    "2https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\n",
    "63 harmful intent in user prompts, identifying safety risks in model responses, and determining when a\n",
    "model appropriately refuses unsafe requests. Central to its development is WILDGUARD MIX3, a\n",
    "meticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\n",
    "adversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\n",
    "for training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\n",
    "for evaluation.\n",
    "The WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\n",
    "TRAIN dataset, enabling it to perform all\n",
    "------------------------\n",
    "output:\n",
    "```plaintext\n",
    "(\"entity\"{{tuple_delimiter}}ARD{{tuple_delimiter}}open-source tool{{tuple_delimiter}}ARD is an innovative open-source tool developed to enhance the safety of interactions with large language models by addressing moderation tasks such as detecting harmful intent, identifying safety risks, and determining appropriate refusals of unsafe requests)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}large language model{{tuple_delimiter}}Large language models (LLMs) are advanced AI models designed to understand and generate human-like text, which ARD aims to interact with safely)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD MIX3 is a meticulously curated dataset comprising 92,000 labeled examples, including benign prompts and adversarial attempts, used for training and evaluating safety measures in language models)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD TRAIN is a subset of the WILDGUARD MIX3 dataset used specifically for training the model on safety measures)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}WILDGUARD TEST{{tuple_delimiter}}dataset{{tuple_delimiter}}WILDGUARD TEST is a subset of the WILDGUARD MIX3 dataset consisting of high-quality human-annotated examples used for evaluating the model's performance)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}MISTRAL-7B{{tuple_delimiter}}large language model{{tuple_delimiter}}Mistral-7B is a language model that the WILDGUARD model is fine-tuned on using the WILDGUARD TRAIN dataset to enhance its safety performance)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}ADVERSARIAL ATTEMPTS{{tuple_delimiter}}adversarial training{{tuple_delimiter}}Adversarial attempts are part of the WILDGUARD MIX3 dataset, used to test and improve the model's ability to handle unsafe or harmful inputs)\n",
    "{{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}SAFETY MEASURES{{tuple_delimiter}}security measures{{tuple_delimiter}}Safety measures are protocols and techniques implemented to ensure that large language models interact safely with users, which ARD and the WILDGUARD dataset aim to enhance)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ARD{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}ARD is designed to enhance the safety of interactions with large language models by addressing critical moderation tasks{{tuple_delimiter}}8)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ARD{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}ARD uses the WILDGUARD MIX3 dataset to train and evaluate its moderation capabilities{{tuple_delimiter}}7)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}WILDGUARD TRAIN is a subset of the WILDGUARD MIX3 dataset used for training{{tuple_delimiter}}9)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD MIX3{{tuple_delimiter}}WILDGUARD TEST{{tuple_delimiter}}WILDGUARD TEST is a subset of the WILDGUARD MIX3 dataset used for evaluation{{tuple_delimiter}}9)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}WILDGUARD TRAIN{{tuple_delimiter}}MISTRAL-7B{{tuple_delimiter}}The WILDGUARD TRAIN dataset is used to fine-tune the Mistral-7B language model{{tuple_delimiter}}8)\n",
    "{{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}ADVERSARIAL ATTEMPTS{{tuple_delimiter}}SAFETY MEASURES{{tuple_delimiter}}Adversarial attempts are used to test and improve safety measures in language models{{tuple_delimiter}}7)\n",
    "{{completion_delimiter}}\n",
    "```\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "entity_types: [large language model, differential privacy, federated learning, healthcare, adversarial training, security measures, open-source tool, dataset, learning rate, AdaGrad, RMSprop, adapter architecture, LoRA, API, model support, evaluation metrics, deployment, Python library, hardware accelerators, hyperparameters, data preprocessing, data imbalance, GPU-based deployment, distributed inference]\n",
    "text: {input_text}\n",
    "######################\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742c48f-043a-4653-ae9a-550d0b929386",
   "metadata": {},
   "source": [
    "**Creating a Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a8e812-85b4-446b-9e54-ca8a227947f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input_text\": texts[25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1176f935-4ffb-4e37-8daa-505edced7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```plaintext\n",
      "(\"entity\"{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}deployment{tuple_delimiter}The deployment environment includes necessary hardware, cloud services, and containerization tools required for deploying models in production.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}API DEVELOPMENT{tuple_delimiter}API{tuple_delimiter}API development involves creating APIs that allow applications to interact with models, facilitating prediction requests and responses.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}CLOUD-BASED LARGE LANGUAGE MODEL INFERENCING{tuple_delimiter}large language model{tuple_delimiter}Cloud-based large language model inferencing uses a pricing model based on the number of tokens processed, charging users according to the volume of text analyzed or generated.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}SELF-HOSTING{tuple_delimiter}deployment{tuple_delimiter}Self-hosting allows organizations to manage their own infrastructure for LLM solutions, providing greater control over resource allocation and cost optimization.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}TOTAL COST OF OWNERSHIP{tuple_delimiter}evaluation metrics{tuple_delimiter}Total cost of ownership is an evaluation metric that considers hardware expenses, maintenance, and operational overheads when comparing cloud-based solutions with self-hosted alternatives.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}AMAZON WEB SERVICES{tuple_delimiter}open-source tool{tuple_delimiter}Amazon Web Services offers various services for deploying large language models, including Amazon Bedrock and Amazon SageMaker, which provide tools for building, training, and deploying models.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}MICROSOFT AZURE{tuple_delimiter}open-source tool{tuple_delimiter}Microsoft Azure provides services like Azure OpenAI Service and Azure Machine Learning for deploying and managing large language models.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}GOOGLE CLOUD PLATFORM{tuple_delimiter}open-source tool{tuple_delimiter}Google Cloud Platform offers Vertex AI and Cloud AI API for deploying large language models and performing various NLP tasks.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}HUGGING FACE{tuple_delimiter}open-source tool{tuple_delimiter}Hugging Face provides an Inference API and Spaces for deploying and managing large language models hosted on its infrastructure.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}GPU-BASED DEPLOYMENTS{tuple_delimiter}GPU-based deployment{tuple_delimiter}GPU-based deployments utilize Graphics Processing Units for fast and efficient inference of large language models, though they require upfront hardware investment.)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}OPTIMISATION TECHNIQUES{tuple_delimiter}evaluation metrics{tuple_delimiter}Optimisation techniques are strategies used to enhance model performance during inference, reduce latency, and manage computational resources effectively.)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}API DEVELOPMENT{tuple_delimiter}The deployment environment is essential for API development as it provides the necessary infrastructure for creating APIs that interact with models.{tuple_delimiter}7)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}CLOUD-BASED LARGE LANGUAGE MODEL INFERENCING{tuple_delimiter}SELF-HOSTING{tuple_delimiter}Cloud-based large language model inferencing and self-hosting are two approaches to deploying models, each with its own cost implications and control over resources.{tuple_delimiter}6)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}TOTAL COST OF OWNERSHIP{tuple_delimiter}CLOUD-BASED LARGE LANGUAGE MODEL INFERENCING{tuple_delimiter}Total cost of ownership is a critical factor when evaluating cloud-based large language model inferencing against self-hosting options.{tuple_delimiter}8)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}AMAZON WEB SERVICES{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}Amazon Web Services provides tools and services that are part of the deployment environment for large language models.{tuple_delimiter}9)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}MICROSOFT AZURE{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}Microsoft Azure offers services that contribute to the deployment environment for large language models.{tuple_delimiter}9)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}GOOGLE CLOUD PLATFORM{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}Google Cloud Platform provides tools that are integral to the deployment environment for large language models.{tuple_delimiter}9)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}HUGGING FACE{tuple_delimiter}DEPLOYMENT ENVIRONMENT{tuple_delimiter}Hugging Face offers services that facilitate the deployment environment for large language models.{tuple_delimiter}9)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}GPU-BASED DEPLOYMENTS{tuple_delimiter}OPTIMISATION TECHNIQUES{tuple_delimiter}Optimisation techniques are necessary for improving the performance of GPU-based deployments of large language models.{tuple_delimiter}8)\n",
      "{completion_delimiter}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd5fa6-626b-4c6b-ad17-84d4d3bc5bf4",
   "metadata": {},
   "source": [
    "We see the extraction of **entities**:\n",
    "\n",
    "`(\"entity\"{tuple_delimiter}EVALUATION METRICS{tuple_delimiter}evaluation metrics{tuple_delimiter}Evaluation metrics are criteria  used to assess the performance of AI models, including metrics like cross-entropy, perplexity, factuality, and context relevance)`\n",
    "\n",
    "As well as **relationships**:\n",
    "\n",
    "`(\"relationship\"{tuple_delimiter}EVALUATION METRICS{tuple_delimiter}CONTEXT RELEVANCE{tuple_delimiter}Context relevance is an evaluation metric that ensures the model uses the most pertinent information for generating responses{tuple_delimiter}8)`\n",
    "\n",
    "Following this, these per chunk subgraphs are merged together - any entities with the same name and type are merged by creating an array of their descriptions. Similarly, any relationships with the same source and target are merged by creating an array of their descriptions. These lists are then summarized one more time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41d036-00da-4dba-8fcf-1cee5b683d52",
   "metadata": {},
   "source": [
    "### **Looking at Final Entities and Relationships**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26613855-7891-4b16-ad84-758f8a0ed8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "human_readable_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_unit_ids",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "frequency",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "degree",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "16961082-9159-4af0-89d8-8ee719910865",
       "rows": [
        [
         "0",
         "3722afa2-33b1-4093-ab3d-b0d658137ea9",
         "0",
         "VENKATESH BALAVADHANI PARTHASARATHY",
         "PERSON",
         "Venkatesh Balavadhani Parthasarathy is one of the authors of the technical report on fine-tuning Large Language Models (LLMs).",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "6871dfbf-33ec-473a-aa6e-f59558cf246a",
         "1",
         "AHTSHAM ZAFAR",
         "PERSON",
         "Ahtsham Zafar is one of the authors of the technical report on fine-tuning Large Language Models (LLMs).",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "d65f2b8f-4032-464f-9546-5021972c5cd7",
         "2",
         "AAFAQ KHAN",
         "PERSON",
         "Aafaq Khan is one of the authors of the technical report on fine-tuning Large Language Models (LLMs).",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "c8cacad0-f473-4807-a0c7-58131731db51",
         "3",
         "ARSALAN SHAHID",
         "PERSON",
         "Arsalan Shahid is one of the authors of the technical report on fine-tuning Large Language Models (LLMs).",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "79fc51c4-ee7a-4bd5-aa1e-77d4747a67d0",
         "4",
         "CEADAR CONNECT GROUP",
         "ORGANIZATION",
         "CeADAR Connect Group is associated with the research on fine-tuning Large Language Models and is part of University College Dublin.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "6",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "9a9cd77f-02b7-44c1-9da8-c1be3db76aa9",
         "5",
         "UNIVERSITY COLLEGE DUBLIN",
         "ORGANIZATION",
         "University College Dublin is the institution where the CeADAR Connect Group is based, focusing on AI research.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "2",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "dafa263c-1e6d-4160-8250-1c8e9a39cf68",
         "6",
         "DUBLIN",
         "GEO",
         "Dublin is the capital city of Ireland, where University College Dublin is located.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "0",
         null,
         null
        ],
        [
         "7",
         "eb0140ee-158a-444f-a22f-8a3b3070a546",
         "7",
         "FINE-TUNING OF LLMS",
         "EVENT",
         "The fine-tuning of Large Language Models (LLMs) is a process discussed in the technical report, involving various methodologies and applications.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "2",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "acd70242-033f-49a7-8792-7b9f1b351e44",
         "8",
         "NATURAL LANGUAGE PROCESSING",
         "EVENT",
         "Natural Language Processing (NLP) is a specialized area within artificial intelligence (AI) that emphasizes the interaction between computers and human language. This field encompasses a variety of techniques and models, including large language models (LLMs), which are designed to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant. NLP plays a crucial role in enabling machines to process and analyze vast amounts of natural language data, facilitating tasks such as translation, sentiment analysis, and conversational agents.\n\nAdditionally, NLP is recognized for its significant advancements, particularly in the context of the DoRA (Dynamic Response Adaptation) framework, which has demonstrated notable improvements in the efficiency and effectiveness of NLP applications. This highlights the ongoing evolution of NLP technologies and their increasing capability to enhance human-computer interaction through more sophisticated language understanding and generation. Overall, NLP represents a vital intersection of technology and linguistics, driving innovations that enhance communication and information processing in various domains.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '72841e45f1ea7ffa4d112b3dcce3fbbe715184d01957f68cb50684c3c9ce9f66bad18a8007ac0eea0c1647f49dee3f09bb09bd07c44bd56e1bb09e4cecbe6360']",
         "2",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "17414fae-ee97-4ca5-8f56-14263ce28e7b",
         "9",
         "GPT-3",
         "EVENT",
         "GPT-3 is a state-of-the-art language model developed by OpenAI, recognized as a notable example of a Large Language Model (LLM). It employs the self-attention mechanism inherent in Transformer architectures, which allows it to effectively manage and process sequential data. This advanced model is designed to be versatile, enabling fine-tuning for a wide range of applications across different domains. Its capabilities make it a significant advancement in the field of natural language processing, showcasing the potential of AI in understanding and generating human-like text.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '30c0acb168bd74b89db6b5849cff8e480192b173392b920f85a4b49d6258bba80d1bc3d37fe2ef06a54712049705461d10fca5ecf32e775d5801ff6770ff0cbf']",
         "2",
         "4",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "c1648488-d2a3-4eb4-8d46-7a72989c7c71",
         "10",
         "GPT-4",
         "EVENT",
         "GPT-4 is a powerful AI model developed by OpenAI, representing an advanced iteration in the series of Large Language Models (LLMs). It is designed for a wide range of applications, including text generation and language understanding, showcasing significant improvements over its predecessor, GPT-3. As an advanced version of the GPT series, GPT-4 builds upon the capabilities of earlier models, enhancing performance in various tasks related to natural language processing. This model is characterized by its ability to generate coherent and contextually relevant text, making it a valuable tool for developers and researchers in the field of artificial intelligence.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '617eb486f0fc61bc3c7b3391fb058d124c4ba9628a5b47900476b8b36e065935be4c392447b3657a7c4a29ec1123ecec5616f77fb5095b3cef34a0f7bdedccef']",
         "2",
         "6",
         "0.0",
         "0.0"
        ],
        [
         "11",
         "b7f3b64a-60d7-4ab2-b1d8-6a4913c67ff4",
         "11",
         "REINFORCEMENT LEARNING FROM HUMAN FEEDBACK",
         "EVENT",
         "Reinforcement Learning from Human Feedback (RLHF) is a sophisticated method designed to enhance the performance of machine learning models by integrating human feedback into the training process. This technique allows for the refinement of models by utilizing human responses, which serve as valuable input to guide the learning algorithms. By leveraging human insights, RLHF aims to align machine learning outcomes more closely with human expectations and preferences, ultimately leading to more effective and user-friendly models. Through this approach, RLHF not only improves the accuracy of predictions but also ensures that the models are better suited to meet the nuanced demands of real-world applications.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '7bc2ccf6d1dad35afde2ff0bd2eb39b7de8873177ec4dc8e29f8767a7559d25b92b4329dbf179f1d7bab6eeceb5632600e62c8720102c7db7337af83f822a4d5']",
         "2",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "12",
         "1ae379e0-c379-4d43-a966-682e1c411196",
         "12",
         "PROMPT ENGINEERING",
         "EVENT",
         "Prompt engineering is a technique used in NLP to design inputs that guide LLMs in generating desired outputs.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "0",
         null,
         null
        ],
        [
         "13",
         "91e999f3-f4f9-44ed-b35f-2b8f120a3f9d",
         "13",
         "MIXTURE OF EXPERTS",
         "EVENT",
         "Mixture of Experts (MoE) is an advanced fine-tuning technique that utilizes specialized networks for improved model performance.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "2",
         "0.0",
         "0.0"
        ],
        [
         "14",
         "2f5256f8-7c58-4de7-aa6a-0e8d76eb73f4",
         "14",
         "MIXTURE OF AGENTS",
         "EVENT",
         "Mixture of Agents (MoA) is a collaborative approach in fine-tuning LLMs that enhances outcomes through multi-agent collaboration.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']",
         "1",
         "1",
         "0.0",
         "0.0"
        ],
        [
         "15",
         "80f3790a-6417-49a9-82e6-79a7456c17cb",
         "15",
         "PROXIMAL POLICY OPTIMISATION",
         "EVENT",
         "Proximal Policy Optimisation (PPO) is a reward-based method utilized in Reinforcement Learning from Human Feedback (RLHF) that focuses on optimizing the reward signal during model training. This innovative approach is designed to align machine learning models more closely with human preferences, ensuring that the models not only perform well in terms of accuracy but also resonate with human values and expectations. The methodology emphasizes the importance of refining the reward mechanisms to enhance the overall effectiveness of the training process, making it a significant topic of discussion in contemporary research on artificial intelligence alignment.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '7bc2ccf6d1dad35afde2ff0bd2eb39b7de8873177ec4dc8e29f8767a7559d25b92b4329dbf179f1d7bab6eeceb5632600e62c8720102c7db7337af83f822a4d5']",
         "2",
         "2",
         "0.0",
         "0.0"
        ],
        [
         "16",
         "a162e769-e888-4fec-9fbc-86be30ab650b",
         "16",
         "DIRECT PREFERENCE OPTIMISATION",
         "EVENT",
         "Direct Preference Optimisation (DPO) is a technique employed in the fine-tuning of large language models, aimed at aligning their outputs with human preferences. This method utilizes preference data and maximum likelihood estimation to enhance the model's ability to generate responses that resonate with human users. DPO serves as an effective approach for ensuring that the outputs of language models are not only accurate but also aligned with the nuanced preferences of users. The technique has been explored in various reports, highlighting its significance in the ongoing efforts to improve the interaction between artificial intelligence and human users. By focusing on aligning model outputs with human preferences, DPO represents a critical advancement in the field of machine learning and natural language processing.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '7bc2ccf6d1dad35afde2ff0bd2eb39b7de8873177ec4dc8e29f8767a7559d25b92b4329dbf179f1d7bab6eeceb5632600e62c8720102c7db7337af83f822a4d5']",
         "2",
         "10",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "062db5ee-7a1f-43da-a78c-6d38d902bf46",
         "17",
         "HYPERPARAMETER TUNING",
         "EVENT",
         "Hyperparameter tuning is a critical process in machine learning that involves adjusting key parameters within a model to enhance its performance. This process is particularly significant in the fine-tuning of large language models (LLMs), where the selection and optimization of hyperparameters can greatly influence both the efficiency and effectiveness of the model. Key settings that are often optimized during hyperparameter tuning include the learning rate and batch size, which are essential for achieving optimal results in various applications, including decision-making processes in data-driven optimization (DPO).\n\nThe methods employed for hyperparameter tuning can vary, with common techniques including grid search and Bayesian optimization. These approaches allow practitioners to systematically explore different combinations of hyperparameters to identify the settings that yield the best performance for a given model. Overall, hyperparameter tuning is an indispensable aspect of model development, as it directly impacts the overall performance and reliability of machine learning systems.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '976adc9b9106c077dabc60e36abc2a79040dda19fff3d6b321b1266bef43bd0e5dd6b8717f7c9fd92dfd2df15ec78ef0bdddd7c558af89ef2591167446f925e8'\n '53f1a27f0f638dd4b00b4bb20ad60047e37dfaeaa091b7571d495c815da4251221c682885d175934e5de7f72c96bd2c14a375d99968d85fc1387d3721e06e063'\n '7bc2ccf6d1dad35afde2ff0bd2eb39b7de8873177ec4dc8e29f8767a7559d25b92b4329dbf179f1d7bab6eeceb5632600e62c8720102c7db7337af83f822a4d5'\n 'fe31f0c502c09a1b13c1fb26743bdbd8d99ca3cb0202c29c44d146dcaaf1c1cbd24c467e410905769b9b980db3155221c178b6310bbca297ec339af7fbe2298f']",
         "5",
         "6",
         "0.0",
         "0.0"
        ],
        [
         "18",
         "a18e6f34-e971-48ac-bebf-fe88d5785eec",
         "18",
         "LOW-RANK ADAPTATION",
         "EVENT",
         "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method designed to optimize model performance while addressing resource constraints. This technique is particularly effective in scenarios where the majority of the model remains frozen, allowing for efficient fine-tuning without the need to retrain the entire model. LoRA achieves this by updating pre-trained weights through the use of low-rank matrices, which helps maintain efficiency and reduces the computational burden typically associated with fine-tuning large models. Overall, LoRA represents a strategic approach to enhancing model capabilities while minimizing resource expenditure.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '72841e45f1ea7ffa4d112b3dcce3fbbe715184d01957f68cb50684c3c9ce9f66bad18a8007ac0eea0c1647f49dee3f09bb09bd07c44bd56e1bb09e4cecbe6360'\n 'a1eef2dcc7a96151a4c17b55fa23420ed362944d245a59c4f03fc42053c0181aff37aebfda47cc376ee6ab6f4d8beaf10dba31fca38a127c65889f8f1a688736']",
         "3",
         "2",
         "0.0",
         "0.0"
        ],
        [
         "19",
         "469a4bfb-237f-4d79-8f70-591f21d03d06",
         "19",
         "DATA PREPARATION",
         "EVENT",
         "Data preparation is a critical process in the context of fine-tuning models, particularly for large language models (LLMs). It encompasses the curation and preparation of datasets, which are essential for effective model training. This stage is vital as it directly influences the performance and accuracy of the models being developed.\n\nThe data preparation process involves several key strategies, including data collection and the management of imbalanced datasets. Proper data collection ensures that the datasets used for training are representative and comprehensive, which is necessary for the model to learn effectively. Additionally, handling imbalanced datasets is crucial, as imbalances can lead to biased models that perform poorly on underrepresented classes.\n\nOverall, data preparation serves as the foundation for successful model fine-tuning, making it an indispensable part of the machine learning pipeline.",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a'\n '30c0acb168bd74b89db6b5849cff8e480192b173392b920f85a4b49d6258bba80d1bc3d37fe2ef06a54712049705461d10fca5ecf32e775d5801ff6770ff0cbf']",
         "2",
         "2",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>frequency</th>\n",
       "      <th>degree</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722afa2-33b1-4093-ab3d-b0d658137ea9</td>\n",
       "      <td>0</td>\n",
       "      <td>VENKATESH BALAVADHANI PARTHASARATHY</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Venkatesh Balavadhani Parthasarathy is one of ...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6871dfbf-33ec-473a-aa6e-f59558cf246a</td>\n",
       "      <td>1</td>\n",
       "      <td>AHTSHAM ZAFAR</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Ahtsham Zafar is one of the authors of the tec...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d65f2b8f-4032-464f-9546-5021972c5cd7</td>\n",
       "      <td>2</td>\n",
       "      <td>AAFAQ KHAN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Aafaq Khan is one of the authors of the techni...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c8cacad0-f473-4807-a0c7-58131731db51</td>\n",
       "      <td>3</td>\n",
       "      <td>ARSALAN SHAHID</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Arsalan Shahid is one of the authors of the te...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79fc51c4-ee7a-4bd5-aa1e-77d4747a67d0</td>\n",
       "      <td>4</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>CeADAR Connect Group is associated with the re...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9a9cd77f-02b7-44c1-9da8-c1be3db76aa9</td>\n",
       "      <td>5</td>\n",
       "      <td>UNIVERSITY COLLEGE DUBLIN</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>University College Dublin is the institution w...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dafa263c-1e6d-4160-8250-1c8e9a39cf68</td>\n",
       "      <td>6</td>\n",
       "      <td>DUBLIN</td>\n",
       "      <td>GEO</td>\n",
       "      <td>Dublin is the capital city of Ireland, where U...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eb0140ee-158a-444f-a22f-8a3b3070a546</td>\n",
       "      <td>7</td>\n",
       "      <td>FINE-TUNING OF LLMS</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>The fine-tuning of Large Language Models (LLMs...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>acd70242-033f-49a7-8792-7b9f1b351e44</td>\n",
       "      <td>8</td>\n",
       "      <td>NATURAL LANGUAGE PROCESSING</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Natural Language Processing (NLP) is a special...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17414fae-ee97-4ca5-8f56-14263ce28e7b</td>\n",
       "      <td>9</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>GPT-3 is a state-of-the-art language model dev...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c1648488-d2a3-4eb4-8d46-7a72989c7c71</td>\n",
       "      <td>10</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>GPT-4 is a powerful AI model developed by Open...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b7f3b64a-60d7-4ab2-b1d8-6a4913c67ff4</td>\n",
       "      <td>11</td>\n",
       "      <td>REINFORCEMENT LEARNING FROM HUMAN FEEDBACK</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Reinforcement Learning from Human Feedback (RL...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1ae379e0-c379-4d43-a966-682e1c411196</td>\n",
       "      <td>12</td>\n",
       "      <td>PROMPT ENGINEERING</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Prompt engineering is a technique used in NLP ...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91e999f3-f4f9-44ed-b35f-2b8f120a3f9d</td>\n",
       "      <td>13</td>\n",
       "      <td>MIXTURE OF EXPERTS</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Mixture of Experts (MoE) is an advanced fine-t...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2f5256f8-7c58-4de7-aa6a-0e8d76eb73f4</td>\n",
       "      <td>14</td>\n",
       "      <td>MIXTURE OF AGENTS</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Mixture of Agents (MoA) is a collaborative app...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80f3790a-6417-49a9-82e6-79a7456c17cb</td>\n",
       "      <td>15</td>\n",
       "      <td>PROXIMAL POLICY OPTIMISATION</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Proximal Policy Optimisation (PPO) is a reward...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a162e769-e888-4fec-9fbc-86be30ab650b</td>\n",
       "      <td>16</td>\n",
       "      <td>DIRECT PREFERENCE OPTIMISATION</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Direct Preference Optimisation (DPO) is a tech...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>062db5ee-7a1f-43da-a78c-6d38d902bf46</td>\n",
       "      <td>17</td>\n",
       "      <td>HYPERPARAMETER TUNING</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Hyperparameter tuning is a critical process in...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a18e6f34-e971-48ac-bebf-fe88d5785eec</td>\n",
       "      <td>18</td>\n",
       "      <td>LOW-RANK ADAPTATION</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Low-Rank Adaptation (LoRA) is a parameter-effi...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>469a4bfb-237f-4d79-8f70-591f21d03d06</td>\n",
       "      <td>19</td>\n",
       "      <td>DATA PREPARATION</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>Data preparation is a critical process in the ...</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  human_readable_id  \\\n",
       "0   3722afa2-33b1-4093-ab3d-b0d658137ea9                  0   \n",
       "1   6871dfbf-33ec-473a-aa6e-f59558cf246a                  1   \n",
       "2   d65f2b8f-4032-464f-9546-5021972c5cd7                  2   \n",
       "3   c8cacad0-f473-4807-a0c7-58131731db51                  3   \n",
       "4   79fc51c4-ee7a-4bd5-aa1e-77d4747a67d0                  4   \n",
       "5   9a9cd77f-02b7-44c1-9da8-c1be3db76aa9                  5   \n",
       "6   dafa263c-1e6d-4160-8250-1c8e9a39cf68                  6   \n",
       "7   eb0140ee-158a-444f-a22f-8a3b3070a546                  7   \n",
       "8   acd70242-033f-49a7-8792-7b9f1b351e44                  8   \n",
       "9   17414fae-ee97-4ca5-8f56-14263ce28e7b                  9   \n",
       "10  c1648488-d2a3-4eb4-8d46-7a72989c7c71                 10   \n",
       "11  b7f3b64a-60d7-4ab2-b1d8-6a4913c67ff4                 11   \n",
       "12  1ae379e0-c379-4d43-a966-682e1c411196                 12   \n",
       "13  91e999f3-f4f9-44ed-b35f-2b8f120a3f9d                 13   \n",
       "14  2f5256f8-7c58-4de7-aa6a-0e8d76eb73f4                 14   \n",
       "15  80f3790a-6417-49a9-82e6-79a7456c17cb                 15   \n",
       "16  a162e769-e888-4fec-9fbc-86be30ab650b                 16   \n",
       "17  062db5ee-7a1f-43da-a78c-6d38d902bf46                 17   \n",
       "18  a18e6f34-e971-48ac-bebf-fe88d5785eec                 18   \n",
       "19  469a4bfb-237f-4d79-8f70-591f21d03d06                 19   \n",
       "\n",
       "                                         title          type  \\\n",
       "0          VENKATESH BALAVADHANI PARTHASARATHY        PERSON   \n",
       "1                                AHTSHAM ZAFAR        PERSON   \n",
       "2                                   AAFAQ KHAN        PERSON   \n",
       "3                               ARSALAN SHAHID        PERSON   \n",
       "4                         CEADAR CONNECT GROUP  ORGANIZATION   \n",
       "5                    UNIVERSITY COLLEGE DUBLIN  ORGANIZATION   \n",
       "6                                       DUBLIN           GEO   \n",
       "7                          FINE-TUNING OF LLMS         EVENT   \n",
       "8                  NATURAL LANGUAGE PROCESSING         EVENT   \n",
       "9                                        GPT-3         EVENT   \n",
       "10                                       GPT-4         EVENT   \n",
       "11  REINFORCEMENT LEARNING FROM HUMAN FEEDBACK         EVENT   \n",
       "12                          PROMPT ENGINEERING         EVENT   \n",
       "13                          MIXTURE OF EXPERTS         EVENT   \n",
       "14                           MIXTURE OF AGENTS         EVENT   \n",
       "15                PROXIMAL POLICY OPTIMISATION         EVENT   \n",
       "16              DIRECT PREFERENCE OPTIMISATION         EVENT   \n",
       "17                       HYPERPARAMETER TUNING         EVENT   \n",
       "18                         LOW-RANK ADAPTATION         EVENT   \n",
       "19                            DATA PREPARATION         EVENT   \n",
       "\n",
       "                                          description  \\\n",
       "0   Venkatesh Balavadhani Parthasarathy is one of ...   \n",
       "1   Ahtsham Zafar is one of the authors of the tec...   \n",
       "2   Aafaq Khan is one of the authors of the techni...   \n",
       "3   Arsalan Shahid is one of the authors of the te...   \n",
       "4   CeADAR Connect Group is associated with the re...   \n",
       "5   University College Dublin is the institution w...   \n",
       "6   Dublin is the capital city of Ireland, where U...   \n",
       "7   The fine-tuning of Large Language Models (LLMs...   \n",
       "8   Natural Language Processing (NLP) is a special...   \n",
       "9   GPT-3 is a state-of-the-art language model dev...   \n",
       "10  GPT-4 is a powerful AI model developed by Open...   \n",
       "11  Reinforcement Learning from Human Feedback (RL...   \n",
       "12  Prompt engineering is a technique used in NLP ...   \n",
       "13  Mixture of Experts (MoE) is an advanced fine-t...   \n",
       "14  Mixture of Agents (MoA) is a collaborative app...   \n",
       "15  Proximal Policy Optimisation (PPO) is a reward...   \n",
       "16  Direct Preference Optimisation (DPO) is a tech...   \n",
       "17  Hyperparameter tuning is a critical process in...   \n",
       "18  Low-Rank Adaptation (LoRA) is a parameter-effi...   \n",
       "19  Data preparation is a critical process in the ...   \n",
       "\n",
       "                                        text_unit_ids  frequency  degree    x  \\\n",
       "0   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       1  0.0   \n",
       "1   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       1  0.0   \n",
       "2   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       1  0.0   \n",
       "3   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       1  0.0   \n",
       "4   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       6  0.0   \n",
       "5   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       2  0.0   \n",
       "6   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       0  NaN   \n",
       "7   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       2  0.0   \n",
       "8   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       1  0.0   \n",
       "9   [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       4  0.0   \n",
       "10  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       6  0.0   \n",
       "11  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       1  0.0   \n",
       "12  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       0  NaN   \n",
       "13  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       2  0.0   \n",
       "14  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          1       1  0.0   \n",
       "15  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       2  0.0   \n",
       "16  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2      10  0.0   \n",
       "17  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          5       6  0.0   \n",
       "18  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          3       2  0.0   \n",
       "19  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...          2       2  0.0   \n",
       "\n",
       "      y  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "5   0.0  \n",
       "6   NaN  \n",
       "7   0.0  \n",
       "8   0.0  \n",
       "9   0.0  \n",
       "10  0.0  \n",
       "11  0.0  \n",
       "12  NaN  \n",
       "13  0.0  \n",
       "14  0.0  \n",
       "15  0.0  \n",
       "16  0.0  \n",
       "17  0.0  \n",
       "18  0.0  \n",
       "19  0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_parquet('./paper/output/entities.parquet')\n",
    "\n",
    "entities.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4bf9fc-bb4e-4546-897b-991236079323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "human_readable_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "combined_degree",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_unit_ids",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "c2137979-7515-4e90-9e97-a6566fba8ce1",
       "rows": [
        [
         "0",
         "66aeff94-c931-47b9-8d05-7a144e1bffed",
         "0",
         "VENKATESH BALAVADHANI PARTHASARATHY",
         "CEADAR CONNECT GROUP",
         "Venkatesh Balavadhani Parthasarathy is affiliated with the CeADAR Connect Group as an author of the report.",
         "8.0",
         "7",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "1",
         "0af75879-f95d-429e-a087-50dd3a55b8fa",
         "1",
         "AHTSHAM ZAFAR",
         "CEADAR CONNECT GROUP",
         "Ahtsham Zafar is affiliated with the CeADAR Connect Group as an author of the report.",
         "8.0",
         "7",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "2",
         "129a277b-6f38-4d5b-b432-7b66bf4828be",
         "2",
         "AAFAQ KHAN",
         "CEADAR CONNECT GROUP",
         "Aafaq Khan is affiliated with the CeADAR Connect Group as an author of the report.",
         "8.0",
         "7",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "3",
         "21b5fc78-0590-4603-846e-e1656fc7412d",
         "3",
         "ARSALAN SHAHID",
         "CEADAR CONNECT GROUP",
         "Arsalan Shahid is affiliated with the CeADAR Connect Group as an author of the report.",
         "8.0",
         "7",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "4",
         "4c4fd723-81e6-4196-889a-660da3be2fd7",
         "4",
         "CEADAR CONNECT GROUP",
         "UNIVERSITY COLLEGE DUBLIN",
         "CeADAR Connect Group is part of University College Dublin, focusing on AI research.",
         "9.0",
         "8",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "5",
         "134dd6fb-b8d3-4061-be15-3de126e2556d",
         "5",
         "CEADAR CONNECT GROUP",
         "FINE-TUNING OF LLMS",
         "The CeADAR Connect Group is involved in the research and development of fine-tuning Large Language Models.",
         "7.0",
         "8",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "6",
         "3cd1d3f1-69f6-4194-bce6-037cb8998805",
         "6",
         "UNIVERSITY COLLEGE DUBLIN",
         "FINE-TUNING OF LLMS",
         "University College Dublin is the institution where the research on fine-tuning Large Language Models is conducted.",
         "1.0",
         "4",
         "['e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd557eeb593f9d202981a480d0f661cd4eec8b74fc3d4601e80e365c5412fa6b39899374b0156520d7e8a']"
        ],
        [
         "7",
         "a5f00fd5-d21d-4698-9c32-0c1c0c72125a",
         "7",
         "GPT-2",
         "BERT",
         "Both GPT-2 and BERT are pre-trained language models that have significantly advanced the field of natural language processing",
         "7.0",
         "5",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "8",
         "4d02bca4-03f9-48d3-8a27-c2d9bfc237e0",
         "8",
         "GPT-2",
         "OPENAI",
         "OpenAI developed the GPT-2 language model, which is a significant advancement in natural language processing",
         "9.0",
         "19",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "9",
         "33f37318-a466-48ea-8ab5-4cba22288d22",
         "9",
         "BERT",
         "GOOGLE",
         "Google developed BERT, a pre-trained language model that has transformed search engine capabilities",
         "9.0",
         "4",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "10",
         "80657e57-f3fe-4377-a608-022edf1934cd",
         "10",
         "GPT-3",
         "GPT-4",
         "GPT-4 is an improved version of GPT-3, building upon its capabilities and performance in language tasks",
         "8.0",
         "10",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "11",
         "94120b54-f9b4-4bb5-acb1-2a2b53467e16",
         "11",
         "GPT-3",
         "PALM",
         "PaLM and GPT-3 are both large language models developed by leading tech companies, focusing on complex language understanding",
         "6.0",
         "38",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "12",
         "f170e7ab-9567-4fc7-965e-7dd6884da5f2",
         "12",
         "GPT-3",
         "LLAMA",
         "LLAMA and GPT-3 are both large language models that aim to improve efficiency and performance in NLP tasks",
         "5.0",
         "20",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "13",
         "a20d97e8-4f5d-41ea-aee5-79edece1d784",
         "13",
         "GPT-3",
         "OPENAI",
         "OpenAI is the organization behind the development of GPT-3, a powerful language model used for various applications",
         "9.0",
         "21",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "14",
         "2cc231bf-a52b-44af-a989-c733b2f93f11",
         "14",
         "GPT-4",
         "OPENAI",
         "OpenAI developed GPT-4, which is an advanced iteration of its language models, improving upon the capabilities of GPT-3",
         "9.0",
         "23",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "15",
         "c88ecf0e-6c39-4311-8d3f-0ad2a698839f",
         "15",
         "NLP",
         "RETRIEVAL-AUGMENTED GENERATION",
         "Retrieval-Augmented Generation is a technique used within the field of NLP to enhance the performance of language models",
         "1.0",
         "4",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "16",
         "c87043ed-ba05-4e99-a839-88a14d1a471d",
         "16",
         "FINE-TUNING",
         "TRANSFER LEARNING",
         "Fine-tuning is a specific application of transfer learning, where a pre-trained model is adapted for a new task",
         "8.0",
         "35",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "17",
         "ffd77d6e-f2b7-43eb-8c77-19b2f7b6109f",
         "17",
         "FINE-TUNING",
         "SUPERVISED FINE-TUNING",
         "Supervised fine-tuning is a method of fine-tuning that uses labeled data to improve model performance",
         "7.0",
         "32",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "18",
         "2aca9ba2-d0c7-429a-9a5f-bb88bc3a9545",
         "18",
         "FINE-TUNING",
         "UNSUPERVISED FINE-TUNING",
         "Unsupervised fine-tuning is another method of fine-tuning that does not require labeled data, focusing on general language understanding",
         "1.0",
         "31",
         "['e85559cf1d772032514c0673126bacb711eda0989e153757662d948f95e54203890ef042689b9f55fbbbcfdb888a4a489980af11822b66b8a3f5ab2a02de9ba9']"
        ],
        [
         "19",
         "13343438-f956-48b9-b42f-e44101a8f45c",
         "19",
         "RETRIEVAL AUGMENTED GENERATION",
         "LLM",
         "RAG enhances LLMs by incorporating external data for improved response accuracy",
         "8.0",
         "57",
         "['c2dff6b66250dd2e17a9e1c3cc53ffb8ee43bbad957ebf0f536302e23b9d39019875e3b35ef35d927be6c25d8ad5f1c301c51c6b48fddff001b2e27788083e71']"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66aeff94-c931-47b9-8d05-7a144e1bffed</td>\n",
       "      <td>0</td>\n",
       "      <td>VENKATESH BALAVADHANI PARTHASARATHY</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>Venkatesh Balavadhani Parthasarathy is affilia...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0af75879-f95d-429e-a087-50dd3a55b8fa</td>\n",
       "      <td>1</td>\n",
       "      <td>AHTSHAM ZAFAR</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>Ahtsham Zafar is affiliated with the CeADAR Co...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129a277b-6f38-4d5b-b432-7b66bf4828be</td>\n",
       "      <td>2</td>\n",
       "      <td>AAFAQ KHAN</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>Aafaq Khan is affiliated with the CeADAR Conne...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21b5fc78-0590-4603-846e-e1656fc7412d</td>\n",
       "      <td>3</td>\n",
       "      <td>ARSALAN SHAHID</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>Arsalan Shahid is affiliated with the CeADAR C...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c4fd723-81e6-4196-889a-660da3be2fd7</td>\n",
       "      <td>4</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>UNIVERSITY COLLEGE DUBLIN</td>\n",
       "      <td>CeADAR Connect Group is part of University Col...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134dd6fb-b8d3-4061-be15-3de126e2556d</td>\n",
       "      <td>5</td>\n",
       "      <td>CEADAR CONNECT GROUP</td>\n",
       "      <td>FINE-TUNING OF LLMS</td>\n",
       "      <td>The CeADAR Connect Group is involved in the re...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3cd1d3f1-69f6-4194-bce6-037cb8998805</td>\n",
       "      <td>6</td>\n",
       "      <td>UNIVERSITY COLLEGE DUBLIN</td>\n",
       "      <td>FINE-TUNING OF LLMS</td>\n",
       "      <td>University College Dublin is the institution w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a5f00fd5-d21d-4698-9c32-0c1c0c72125a</td>\n",
       "      <td>7</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Both GPT-2 and BERT are pre-trained language m...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4d02bca4-03f9-48d3-8a27-c2d9bfc237e0</td>\n",
       "      <td>8</td>\n",
       "      <td>GPT-2</td>\n",
       "      <td>OPENAI</td>\n",
       "      <td>OpenAI developed the GPT-2 language model, whi...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33f37318-a466-48ea-8ab5-4cba22288d22</td>\n",
       "      <td>9</td>\n",
       "      <td>BERT</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>Google developed BERT, a pre-trained language ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>80657e57-f3fe-4377-a608-022edf1934cd</td>\n",
       "      <td>10</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>GPT-4 is an improved version of GPT-3, buildin...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94120b54-f9b4-4bb5-acb1-2a2b53467e16</td>\n",
       "      <td>11</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>PALM</td>\n",
       "      <td>PaLM and GPT-3 are both large language models ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>f170e7ab-9567-4fc7-965e-7dd6884da5f2</td>\n",
       "      <td>12</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>LLAMA and GPT-3 are both large language models...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a20d97e8-4f5d-41ea-aee5-79edece1d784</td>\n",
       "      <td>13</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>OPENAI</td>\n",
       "      <td>OpenAI is the organization behind the developm...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2cc231bf-a52b-44af-a989-c733b2f93f11</td>\n",
       "      <td>14</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>OPENAI</td>\n",
       "      <td>OpenAI developed GPT-4, which is an advanced i...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>c88ecf0e-6c39-4311-8d3f-0ad2a698839f</td>\n",
       "      <td>15</td>\n",
       "      <td>NLP</td>\n",
       "      <td>RETRIEVAL-AUGMENTED GENERATION</td>\n",
       "      <td>Retrieval-Augmented Generation is a technique ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c87043ed-ba05-4e99-a839-88a14d1a471d</td>\n",
       "      <td>16</td>\n",
       "      <td>FINE-TUNING</td>\n",
       "      <td>TRANSFER LEARNING</td>\n",
       "      <td>Fine-tuning is a specific application of trans...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ffd77d6e-f2b7-43eb-8c77-19b2f7b6109f</td>\n",
       "      <td>17</td>\n",
       "      <td>FINE-TUNING</td>\n",
       "      <td>SUPERVISED FINE-TUNING</td>\n",
       "      <td>Supervised fine-tuning is a method of fine-tun...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2aca9ba2-d0c7-429a-9a5f-bb88bc3a9545</td>\n",
       "      <td>18</td>\n",
       "      <td>FINE-TUNING</td>\n",
       "      <td>UNSUPERVISED FINE-TUNING</td>\n",
       "      <td>Unsupervised fine-tuning is another method of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>[e85559cf1d772032514c0673126bacb711eda0989e153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13343438-f956-48b9-b42f-e44101a8f45c</td>\n",
       "      <td>19</td>\n",
       "      <td>RETRIEVAL AUGMENTED GENERATION</td>\n",
       "      <td>LLM</td>\n",
       "      <td>RAG enhances LLMs by incorporating external da...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57</td>\n",
       "      <td>[c2dff6b66250dd2e17a9e1c3cc53ffb8ee43bbad957eb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  human_readable_id  \\\n",
       "0   66aeff94-c931-47b9-8d05-7a144e1bffed                  0   \n",
       "1   0af75879-f95d-429e-a087-50dd3a55b8fa                  1   \n",
       "2   129a277b-6f38-4d5b-b432-7b66bf4828be                  2   \n",
       "3   21b5fc78-0590-4603-846e-e1656fc7412d                  3   \n",
       "4   4c4fd723-81e6-4196-889a-660da3be2fd7                  4   \n",
       "5   134dd6fb-b8d3-4061-be15-3de126e2556d                  5   \n",
       "6   3cd1d3f1-69f6-4194-bce6-037cb8998805                  6   \n",
       "7   a5f00fd5-d21d-4698-9c32-0c1c0c72125a                  7   \n",
       "8   4d02bca4-03f9-48d3-8a27-c2d9bfc237e0                  8   \n",
       "9   33f37318-a466-48ea-8ab5-4cba22288d22                  9   \n",
       "10  80657e57-f3fe-4377-a608-022edf1934cd                 10   \n",
       "11  94120b54-f9b4-4bb5-acb1-2a2b53467e16                 11   \n",
       "12  f170e7ab-9567-4fc7-965e-7dd6884da5f2                 12   \n",
       "13  a20d97e8-4f5d-41ea-aee5-79edece1d784                 13   \n",
       "14  2cc231bf-a52b-44af-a989-c733b2f93f11                 14   \n",
       "15  c88ecf0e-6c39-4311-8d3f-0ad2a698839f                 15   \n",
       "16  c87043ed-ba05-4e99-a839-88a14d1a471d                 16   \n",
       "17  ffd77d6e-f2b7-43eb-8c77-19b2f7b6109f                 17   \n",
       "18  2aca9ba2-d0c7-429a-9a5f-bb88bc3a9545                 18   \n",
       "19  13343438-f956-48b9-b42f-e44101a8f45c                 19   \n",
       "\n",
       "                                 source                          target  \\\n",
       "0   VENKATESH BALAVADHANI PARTHASARATHY            CEADAR CONNECT GROUP   \n",
       "1                         AHTSHAM ZAFAR            CEADAR CONNECT GROUP   \n",
       "2                            AAFAQ KHAN            CEADAR CONNECT GROUP   \n",
       "3                        ARSALAN SHAHID            CEADAR CONNECT GROUP   \n",
       "4                  CEADAR CONNECT GROUP       UNIVERSITY COLLEGE DUBLIN   \n",
       "5                  CEADAR CONNECT GROUP             FINE-TUNING OF LLMS   \n",
       "6             UNIVERSITY COLLEGE DUBLIN             FINE-TUNING OF LLMS   \n",
       "7                                 GPT-2                            BERT   \n",
       "8                                 GPT-2                          OPENAI   \n",
       "9                                  BERT                          GOOGLE   \n",
       "10                                GPT-3                           GPT-4   \n",
       "11                                GPT-3                            PALM   \n",
       "12                                GPT-3                           LLAMA   \n",
       "13                                GPT-3                          OPENAI   \n",
       "14                                GPT-4                          OPENAI   \n",
       "15                                  NLP  RETRIEVAL-AUGMENTED GENERATION   \n",
       "16                          FINE-TUNING               TRANSFER LEARNING   \n",
       "17                          FINE-TUNING          SUPERVISED FINE-TUNING   \n",
       "18                          FINE-TUNING        UNSUPERVISED FINE-TUNING   \n",
       "19       RETRIEVAL AUGMENTED GENERATION                             LLM   \n",
       "\n",
       "                                          description  weight  \\\n",
       "0   Venkatesh Balavadhani Parthasarathy is affilia...     8.0   \n",
       "1   Ahtsham Zafar is affiliated with the CeADAR Co...     8.0   \n",
       "2   Aafaq Khan is affiliated with the CeADAR Conne...     8.0   \n",
       "3   Arsalan Shahid is affiliated with the CeADAR C...     8.0   \n",
       "4   CeADAR Connect Group is part of University Col...     9.0   \n",
       "5   The CeADAR Connect Group is involved in the re...     7.0   \n",
       "6   University College Dublin is the institution w...     1.0   \n",
       "7   Both GPT-2 and BERT are pre-trained language m...     7.0   \n",
       "8   OpenAI developed the GPT-2 language model, whi...     9.0   \n",
       "9   Google developed BERT, a pre-trained language ...     9.0   \n",
       "10  GPT-4 is an improved version of GPT-3, buildin...     8.0   \n",
       "11  PaLM and GPT-3 are both large language models ...     6.0   \n",
       "12  LLAMA and GPT-3 are both large language models...     5.0   \n",
       "13  OpenAI is the organization behind the developm...     9.0   \n",
       "14  OpenAI developed GPT-4, which is an advanced i...     9.0   \n",
       "15  Retrieval-Augmented Generation is a technique ...     1.0   \n",
       "16  Fine-tuning is a specific application of trans...     8.0   \n",
       "17  Supervised fine-tuning is a method of fine-tun...     7.0   \n",
       "18  Unsupervised fine-tuning is another method of ...     1.0   \n",
       "19  RAG enhances LLMs by incorporating external da...     8.0   \n",
       "\n",
       "    combined_degree                                      text_unit_ids  \n",
       "0                 7  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "1                 7  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "2                 7  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "3                 7  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "4                 8  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "5                 8  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "6                 4  [e23f40dc2b1f2299af343eaf1bb144930cb2b5047b7cd...  \n",
       "7                 5  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "8                19  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "9                 4  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "10               10  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "11               38  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "12               20  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "13               21  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "14               23  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "15                4  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "16               35  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "17               32  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "18               31  [e85559cf1d772032514c0673126bacb711eda0989e153...  \n",
       "19               57  [c2dff6b66250dd2e17a9e1c3cc53ffb8ee43bbad957eb...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships = pd.read_parquet('./paper/output/relationships.parquet')\n",
    "\n",
    "relationships.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c77836-f9ae-4602-a4d0-760915248e0a",
   "metadata": {},
   "source": [
    "### **Community Detection & Node Embedding**\n",
    "\n",
    "<img src=\"./media/leidan.png\" width=600>\n",
    "\n",
    "After we have our basic graph with entities and relationships, we analyze its structure in two ways. Community Detection uses the [Leiden algorithm](https://en.wikipedia.org/wiki/Leiden_algorithm) to find explicit groupings in the graph, creating a hierarchy of related entities. The lower in the hierarchy, the more granular the community. Node Embedding uses [Node2Vec](https://arxiv.org/abs/1607.00653) to create vector representations of each entity, capturing implicit relationships in the graph structure. These complementary approaches let us understand both obvious connections through communities and subtle patterns through embeddings.\n",
    "\n",
    "Combining all of this with our relationships gives us our final nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "058d8f6d-6eb7-45fe-bf1c-e28055a683c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = pd.read_parquet('./paper/output/create_final_nodes.parquet')\n",
    "\n",
    "# nodes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b12c9-fc5c-47cf-a597-6c6bfb3177ee",
   "metadata": {},
   "source": [
    "At this step the graph is effectively created, however we can introduce a few extra steps that will allow us to do some advanced retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8ed95-677b-405e-82d3-6fb1a3f1917c",
   "metadata": {},
   "source": [
    "### Community Report Generation & Summarization\n",
    "\n",
    "Now that we have clear community grouping, we can aggregate the main concepts across hierarchical node communities with another generation step, and a shorthand summary of that summary. Similar to the nodes, these summaries are also ran through an embedding model and stored in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702e5559-ed4e-4964-bf72-113912974102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "human_readable_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "parent",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "children",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rank",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rating_explanation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "findings",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "full_content_json",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "period",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "size",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "08314c09-0e2e-4ec6-b667-bc10538b5761",
       "rows": [
        [
         "0",
         "082bb8c74db541a48fb16acfe85e3c67",
         "72",
         "72",
         "2",
         "21",
         "[]",
         "PyTorch and Large Language Models Community",
         "The community centers around PyTorch, a leading machine learning library, and its relationship with Large Language Models (LLMs) and associated entities like Hugging Face and Meta. These entities collaborate to advance the development and ethical deployment of AI technologies, particularly in natural language processing.",
         "# PyTorch and Large Language Models Community\n\nThe community centers around PyTorch, a leading machine learning library, and its relationship with Large Language Models (LLMs) and associated entities like Hugging Face and Meta. These entities collaborate to advance the development and ethical deployment of AI technologies, particularly in natural language processing.\n\n## PyTorch as a foundational framework for LLMs\n\nPyTorch is a widely used open-source machine learning library that plays a crucial role in the development and training of Large Language Models (LLMs). It provides a flexible platform that supports both the initialization and fine-tuning of these models, making it integral to the field of natural language processing. The framework's capabilities enable researchers and developers to create sophisticated LLMs that can understand and generate human-like text, which is essential for various applications, from chatbots to advanced AI systems. This foundational role underscores PyTorch's significance in the ongoing evolution of artificial intelligence technologies [Data: Entities (99); Relationships (76)].\n\n## Hugging Face's contribution to LLM accessibility\n\nHugging Face is a prominent company that specializes in natural language processing and provides tools and libraries specifically designed for working with LLMs. Its comprehensive library facilitates the use of pre-trained models and tokenizers, making it easier for developers and researchers to implement advanced NLP techniques in their projects. Additionally, Hugging Face offers an Inference API that simplifies the deployment and management of LLMs, allowing users to leverage these powerful models without needing extensive expertise in machine learning. This commitment to accessibility and usability significantly contributes to the advancement of NLP technologies [Data: Entities (110); Relationships (93)].\n\n## Ethical considerations in LLM deployment\n\nEthical considerations are paramount in the deployment of LLMs, as they address critical issues related to the responsible and fair use of AI technologies. This includes the potential for biases in training data, which can lead to models that inadvertently amplify these biases. The community emphasizes the importance of scrutinizing data used in model training to mitigate harmful or biased content. Furthermore, ethical considerations extend to the moral implications of deploying AI technologies, ensuring that AI systems are used fairly and justly, thereby fostering trust and accountability in AI applications [Data: Entities (97); Relationships (302)].\n\n## Meta's advancements in LLM technology\n\nMeta is recognized for its significant contributions to both social media and artificial intelligence, particularly through its LLaMA project, which focuses on adapting LLMs for various tasks. This initiative reflects Meta's commitment to advancing AI and machine learning, showcasing its expertise in developing sophisticated algorithms that can understand and generate human-like text. The integration of advanced AI technologies into Meta's social media platforms enhances user experience and engagement, demonstrating the company's dual focus on social connectivity and technological innovation [Data: Entities (398); Relationships (446)].\n\n## The role of evaluation metrics in LLM performance\n\nEvaluation metrics are essential for assessing the performance of LLMs and ensuring quality outputs. Metrics such as perplexity, factuality, and completeness are critical for evaluating how well LLMs predict the next word in a sequence, the accuracy of their outputs, and whether they fully address user queries based on the provided context. These metrics help identify low-quality outputs and ensure that LLMs produce coherent and contextually relevant text, which is vital for applications where misinformation could have serious consequences [Data: Entities (242, 243, 246); Relationships (221, 222, 226)].",
         "8.0",
         "The impact severity rating is high due to the significant influence of LLMs on various applications and the ethical implications of their deployment.",
         "[{'explanation': \"PyTorch is a widely used open-source machine learning library that plays a crucial role in the development and training of Large Language Models (LLMs). It provides a flexible platform that supports both the initialization and fine-tuning of these models, making it integral to the field of natural language processing. The framework's capabilities enable researchers and developers to create sophisticated LLMs that can understand and generate human-like text, which is essential for various applications, from chatbots to advanced AI systems. This foundational role underscores PyTorch's significance in the ongoing evolution of artificial intelligence technologies [Data: Entities (99); Relationships (76)].\", 'summary': 'PyTorch as a foundational framework for LLMs'}\n {'explanation': 'Hugging Face is a prominent company that specializes in natural language processing and provides tools and libraries specifically designed for working with LLMs. Its comprehensive library facilitates the use of pre-trained models and tokenizers, making it easier for developers and researchers to implement advanced NLP techniques in their projects. Additionally, Hugging Face offers an Inference API that simplifies the deployment and management of LLMs, allowing users to leverage these powerful models without needing extensive expertise in machine learning. This commitment to accessibility and usability significantly contributes to the advancement of NLP technologies [Data: Entities (110); Relationships (93)].', 'summary': \"Hugging Face's contribution to LLM accessibility\"}\n {'explanation': 'Ethical considerations are paramount in the deployment of LLMs, as they address critical issues related to the responsible and fair use of AI technologies. This includes the potential for biases in training data, which can lead to models that inadvertently amplify these biases. The community emphasizes the importance of scrutinizing data used in model training to mitigate harmful or biased content. Furthermore, ethical considerations extend to the moral implications of deploying AI technologies, ensuring that AI systems are used fairly and justly, thereby fostering trust and accountability in AI applications [Data: Entities (97); Relationships (302)].', 'summary': 'Ethical considerations in LLM deployment'}\n {'explanation': \"Meta is recognized for its significant contributions to both social media and artificial intelligence, particularly through its LLaMA project, which focuses on adapting LLMs for various tasks. This initiative reflects Meta's commitment to advancing AI and machine learning, showcasing its expertise in developing sophisticated algorithms that can understand and generate human-like text. The integration of advanced AI technologies into Meta's social media platforms enhances user experience and engagement, demonstrating the company's dual focus on social connectivity and technological innovation [Data: Entities (398); Relationships (446)].\", 'summary': \"Meta's advancements in LLM technology\"}\n {'explanation': 'Evaluation metrics are essential for assessing the performance of LLMs and ensuring quality outputs. Metrics such as perplexity, factuality, and completeness are critical for evaluating how well LLMs predict the next word in a sequence, the accuracy of their outputs, and whether they fully address user queries based on the provided context. These metrics help identify low-quality outputs and ensure that LLMs produce coherent and contextually relevant text, which is vital for applications where misinformation could have serious consequences [Data: Entities (242, 243, 246); Relationships (221, 222, 226)].', 'summary': 'The role of evaluation metrics in LLM performance'}]",
         "{\n    \"title\": \"PyTorch and Large Language Models Community\",\n    \"summary\": \"The community centers around PyTorch, a leading machine learning library, and its relationship with Large Language Models (LLMs) and associated entities like Hugging Face and Meta. These entities collaborate to advance the development and ethical deployment of AI technologies, particularly in natural language processing.\",\n    \"findings\": [\n        {\n            \"summary\": \"PyTorch as a foundational framework for LLMs\",\n            \"explanation\": \"PyTorch is a widely used open-source machine learning library that plays a crucial role in the development and training of Large Language Models (LLMs). It provides a flexible platform that supports both the initialization and fine-tuning of these models, making it integral to the field of natural language processing. The framework's capabilities enable researchers and developers to create sophisticated LLMs that can understand and generate human-like text, which is essential for various applications, from chatbots to advanced AI systems. This foundational role underscores PyTorch's significance in the ongoing evolution of artificial intelligence technologies [Data: Entities (99); Relationships (76)].\"\n        },\n        {\n            \"summary\": \"Hugging Face's contribution to LLM accessibility\",\n            \"explanation\": \"Hugging Face is a prominent company that specializes in natural language processing and provides tools and libraries specifically designed for working with LLMs. Its comprehensive library facilitates the use of pre-trained models and tokenizers, making it easier for developers and researchers to implement advanced NLP techniques in their projects. Additionally, Hugging Face offers an Inference API that simplifies the deployment and management of LLMs, allowing users to leverage these powerful models without needing extensive expertise in machine learning. This commitment to accessibility and usability significantly contributes to the advancement of NLP technologies [Data: Entities (110); Relationships (93)].\"\n        },\n        {\n            \"summary\": \"Ethical considerations in LLM deployment\",\n            \"explanation\": \"Ethical considerations are paramount in the deployment of LLMs, as they address critical issues related to the responsible and fair use of AI technologies. This includes the potential for biases in training data, which can lead to models that inadvertently amplify these biases. The community emphasizes the importance of scrutinizing data used in model training to mitigate harmful or biased content. Furthermore, ethical considerations extend to the moral implications of deploying AI technologies, ensuring that AI systems are used fairly and justly, thereby fostering trust and accountability in AI applications [Data: Entities (97); Relationships (302)].\"\n        },\n        {\n            \"summary\": \"Meta's advancements in LLM technology\",\n            \"explanation\": \"Meta is recognized for its significant contributions to both social media and artificial intelligence, particularly through its LLaMA project, which focuses on adapting LLMs for various tasks. This initiative reflects Meta's commitment to advancing AI and machine learning, showcasing its expertise in developing sophisticated algorithms that can understand and generate human-like text. The integration of advanced AI technologies into Meta's social media platforms enhances user experience and engagement, demonstrating the company's dual focus on social connectivity and technological innovation [Data: Entities (398); Relationships (446)].\"\n        },\n        {\n            \"summary\": \"The role of evaluation metrics in LLM performance\",\n            \"explanation\": \"Evaluation metrics are essential for assessing the performance of LLMs and ensuring quality outputs. Metrics such as perplexity, factuality, and completeness are critical for evaluating how well LLMs predict the next word in a sequence, the accuracy of their outputs, and whether they fully address user queries based on the provided context. These metrics help identify low-quality outputs and ensure that LLMs produce coherent and contextually relevant text, which is vital for applications where misinformation could have serious consequences [Data: Entities (242, 243, 246); Relationships (221, 222, 226)].\"\n        }\n    ],\n    \"rating\": 8.0,\n    \"rating_explanation\": \"The impact severity rating is high due to the significant influence of LLMs on various applications and the ethical implications of their deployment.\"\n}",
         "2025-12-14",
         "26"
        ],
        [
         "1",
         "cf5a75cc720e4ed29547f2c43dd86ea0",
         "73",
         "73",
         "2",
         "21",
         "[]",
         "Validation Loops and Trends in Validation Metrics",
         "The community focuses on the relationship between Validation Loops and Trends in Validation Metrics, which are essential for evaluating model performance in machine learning. Validation Loops provide a structured approach to assess metrics, while Trends in Validation Metrics offer insights into model generalization over training epochs.",
         "# Validation Loops and Trends in Validation Metrics\n\nThe community focuses on the relationship between Validation Loops and Trends in Validation Metrics, which are essential for evaluating model performance in machine learning. Validation Loops provide a structured approach to assess metrics, while Trends in Validation Metrics offer insights into model generalization over training epochs.\n\n## Importance of Validation Loops\n\nValidation Loops are critical in the machine learning process as they provide an unbiased evaluation of model performance. By assessing metrics at the end of each training epoch, they ensure that the model is not overfitting and is capable of generalizing well to unseen data. This structured evaluation helps in making informed decisions about model adjustments and improvements. [Data: Entities (259)]\n\n## Role of Trends in Validation Metrics\n\nTrends in Validation Metrics are analyzed to monitor model performance and generalization over training epochs. This analysis is vital for understanding how well a model is learning and adapting over time. By tracking these trends, practitioners can identify potential issues early, such as overfitting or underfitting, and take corrective actions to enhance model performance. [Data: Entities (260)]\n\n## Interconnection between Validation Loops and Trends\n\nThe relationship between Validation Loops and Trends in Validation Metrics is significant, as trends are monitored through validation loops to assess model performance. This interconnectedness ensures that the evaluation process is comprehensive, allowing for a continuous feedback loop that informs model training and adjustments. The combined degree of 3 indicates a strong relationship that enhances the overall effectiveness of model evaluation. [Data: Relationships (243)]\n\n## Impact on Model Development\n\nThe integration of Validation Loops and Trends in Validation Metrics has a profound impact on model development. By utilizing these tools, data scientists can refine their models more effectively, leading to improved accuracy and reliability. This systematic approach to monitoring and evaluation is essential in the fast-paced field of machine learning, where model performance can significantly influence outcomes. [Data: Relationships (243)]",
         "7.0",
         "The impact severity rating is high due to the critical role that validation processes play in ensuring the reliability and effectiveness of machine learning models.",
         "[{'explanation': 'Validation Loops are critical in the machine learning process as they provide an unbiased evaluation of model performance. By assessing metrics at the end of each training epoch, they ensure that the model is not overfitting and is capable of generalizing well to unseen data. This structured evaluation helps in making informed decisions about model adjustments and improvements. [Data: Entities (259)]', 'summary': 'Importance of Validation Loops'}\n {'explanation': 'Trends in Validation Metrics are analyzed to monitor model performance and generalization over training epochs. This analysis is vital for understanding how well a model is learning and adapting over time. By tracking these trends, practitioners can identify potential issues early, such as overfitting or underfitting, and take corrective actions to enhance model performance. [Data: Entities (260)]', 'summary': 'Role of Trends in Validation Metrics'}\n {'explanation': 'The relationship between Validation Loops and Trends in Validation Metrics is significant, as trends are monitored through validation loops to assess model performance. This interconnectedness ensures that the evaluation process is comprehensive, allowing for a continuous feedback loop that informs model training and adjustments. The combined degree of 3 indicates a strong relationship that enhances the overall effectiveness of model evaluation. [Data: Relationships (243)]', 'summary': 'Interconnection between Validation Loops and Trends'}\n {'explanation': 'The integration of Validation Loops and Trends in Validation Metrics has a profound impact on model development. By utilizing these tools, data scientists can refine their models more effectively, leading to improved accuracy and reliability. This systematic approach to monitoring and evaluation is essential in the fast-paced field of machine learning, where model performance can significantly influence outcomes. [Data: Relationships (243)]', 'summary': 'Impact on Model Development'}]",
         "{\n    \"title\": \"Validation Loops and Trends in Validation Metrics\",\n    \"summary\": \"The community focuses on the relationship between Validation Loops and Trends in Validation Metrics, which are essential for evaluating model performance in machine learning. Validation Loops provide a structured approach to assess metrics, while Trends in Validation Metrics offer insights into model generalization over training epochs.\",\n    \"findings\": [\n        {\n            \"summary\": \"Importance of Validation Loops\",\n            \"explanation\": \"Validation Loops are critical in the machine learning process as they provide an unbiased evaluation of model performance. By assessing metrics at the end of each training epoch, they ensure that the model is not overfitting and is capable of generalizing well to unseen data. This structured evaluation helps in making informed decisions about model adjustments and improvements. [Data: Entities (259)]\"\n        },\n        {\n            \"summary\": \"Role of Trends in Validation Metrics\",\n            \"explanation\": \"Trends in Validation Metrics are analyzed to monitor model performance and generalization over training epochs. This analysis is vital for understanding how well a model is learning and adapting over time. By tracking these trends, practitioners can identify potential issues early, such as overfitting or underfitting, and take corrective actions to enhance model performance. [Data: Entities (260)]\"\n        },\n        {\n            \"summary\": \"Interconnection between Validation Loops and Trends\",\n            \"explanation\": \"The relationship between Validation Loops and Trends in Validation Metrics is significant, as trends are monitored through validation loops to assess model performance. This interconnectedness ensures that the evaluation process is comprehensive, allowing for a continuous feedback loop that informs model training and adjustments. The combined degree of 3 indicates a strong relationship that enhances the overall effectiveness of model evaluation. [Data: Relationships (243)]\"\n        },\n        {\n            \"summary\": \"Impact on Model Development\",\n            \"explanation\": \"The integration of Validation Loops and Trends in Validation Metrics has a profound impact on model development. By utilizing these tools, data scientists can refine their models more effectively, leading to improved accuracy and reliability. This systematic approach to monitoring and evaluation is essential in the fast-paced field of machine learning, where model performance can significantly influence outcomes. [Data: Relationships (243)]\"\n        }\n    ],\n    \"rating\": 7.0,\n    \"rating_explanation\": \"The impact severity rating is high due to the critical role that validation processes play in ensuring the reliability and effectiveness of machine learning models.\"\n}",
         "2025-12-14",
         "2"
        ],
        [
         "2",
         "a91f21e9e4ed4ab5a0e927b72f1b10b2",
         "74",
         "74",
         "2",
         "31",
         "[]",
         "HuggingFace and AI Safety Models",
         "The community centers around HuggingFace, a leading entity in artificial intelligence, particularly in natural language processing. HuggingFace collaborates with various AI safety models such as Llama Guard 2, Llama Guard 3, and ShieldGemma, enhancing the safety and usability of large language models. The interconnectedness of these entities highlights a robust ecosystem focused on responsible AI development and optimization.",
         "# HuggingFace and AI Safety Models\n\nThe community centers around HuggingFace, a leading entity in artificial intelligence, particularly in natural language processing. HuggingFace collaborates with various AI safety models such as Llama Guard 2, Llama Guard 3, and ShieldGemma, enhancing the safety and usability of large language models. The interconnectedness of these entities highlights a robust ecosystem focused on responsible AI development and optimization.\n\n## HuggingFace as a central player in AI development\n\nHuggingFace is a prominent company recognized for its significant contributions to artificial intelligence, especially in natural language processing (NLP) and machine learning. The company is best known for its Transformers library, which serves as a foundational tool for fine-tuning large language models (LLMs). HuggingFace's commitment to collaboration and safety is evident through its partnerships with various organizations to evaluate and ensure the safety standards of AI models. This central role positions HuggingFace as a key player in the ongoing development of AI technologies, making it a vital entity within this community. [Data: Entities (98); Relationships (73, 193, 254, 255, 256)]\n\n## Integration of Direct Preference Optimization (DPO)\n\nDirect Preference Optimization (DPO) is a method supported by HuggingFace to align language models with human preferences. This approach simplifies the process of incorporating human feedback into model training, making it more accessible for developers. The integration of DPO within HuggingFace's libraries enhances the usability of their models, allowing for more effective alignment with user expectations and preferences. This capability is crucial for the development of responsible AI applications that prioritize user experience. [Data: Entities (213); Relationships (193)]\n\n## Llama Guard models for AI safety\n\nLlama Guard 2 and Llama Guard 3 are safeguard models built on LLMs, focusing on managing risks in conversational AI applications. These models are evaluated and made accessible through HuggingFace's platform, indicating a strong commitment to AI safety. The introduction of new categories for safety evaluation in Llama Guard 3 reflects the evolving landscape of AI interactions and the need for robust safety measures. By incorporating these models, HuggingFace enhances the reliability of AI applications, ensuring safer interactions between users and AI systems. [Data: Entities (284, 285); Relationships (254, 255)]\n\n## ShieldGemma's role in content moderation\n\nShieldGemma is an advanced content moderation model available on HuggingFace, designed to enhance safety and reliability in interactions between LLMs and users. This model plays a critical role in ensuring that AI-generated content adheres to safety standards, thereby reducing the risk of harmful outputs. The availability of ShieldGemma on HuggingFace's platform underscores the company's dedication to providing tools that promote responsible AI usage and mitigate risks associated with AI-generated content. [Data: Entities (286); Relationships (256)]\n\n## SetFit tool for model fine-tuning\n\nSetFit is a tool developed by HuggingFace that allows users to fine-tune models with minimal coding and data requirements. This tool democratizes access to advanced model training techniques, enabling a broader audience to leverage state-of-the-art AI capabilities. By simplifying the fine-tuning process, SetFit empowers developers and researchers to optimize their models effectively, contributing to the overall growth of the AI community. [Data: Entities (363); Relationships (337)]\n\n## LangChain framework for application development\n\nLangChain is a framework that facilitates the development of applications using language models, and HuggingFace's models can be utilized within this framework. This integration allows developers to create sophisticated applications that leverage the capabilities of HuggingFace's models, enhancing the versatility and applicability of AI technologies. The collaboration between HuggingFace and LangChain exemplifies the community's focus on innovation and the development of practical AI solutions. [Data: Entities (370); Relationships (340)]\n\n## Optimum suite for model optimization\n\nOptimum is a comprehensive suite of optimization tools developed by HuggingFace, specifically designed to enhance the performance and efficiency of large language models. By implementing advanced strategies such as pruning and model distillation, Optimum aims to facilitate the deployment of LLMs across various hardware platforms. This focus on optimization not only boosts the performance of LLMs but also makes them more accessible and practical for a wide range of applications, reinforcing HuggingFace's position as a leader in the AI community. [Data: Entities (373); Relationships (354)]",
         "8.5",
         "The impact severity rating is high due to the significant influence of HuggingFace and its associated safety models on the development and deployment of AI technologies.",
         "[{'explanation': \"HuggingFace is a prominent company recognized for its significant contributions to artificial intelligence, especially in natural language processing (NLP) and machine learning. The company is best known for its Transformers library, which serves as a foundational tool for fine-tuning large language models (LLMs). HuggingFace's commitment to collaboration and safety is evident through its partnerships with various organizations to evaluate and ensure the safety standards of AI models. This central role positions HuggingFace as a key player in the ongoing development of AI technologies, making it a vital entity within this community. [Data: Entities (98); Relationships (73, 193, 254, 255, 256)]\", 'summary': 'HuggingFace as a central player in AI development'}\n {'explanation': \"Direct Preference Optimization (DPO) is a method supported by HuggingFace to align language models with human preferences. This approach simplifies the process of incorporating human feedback into model training, making it more accessible for developers. The integration of DPO within HuggingFace's libraries enhances the usability of their models, allowing for more effective alignment with user expectations and preferences. This capability is crucial for the development of responsible AI applications that prioritize user experience. [Data: Entities (213); Relationships (193)]\", 'summary': 'Integration of Direct Preference Optimization (DPO)'}\n {'explanation': \"Llama Guard 2 and Llama Guard 3 are safeguard models built on LLMs, focusing on managing risks in conversational AI applications. These models are evaluated and made accessible through HuggingFace's platform, indicating a strong commitment to AI safety. The introduction of new categories for safety evaluation in Llama Guard 3 reflects the evolving landscape of AI interactions and the need for robust safety measures. By incorporating these models, HuggingFace enhances the reliability of AI applications, ensuring safer interactions between users and AI systems. [Data: Entities (284, 285); Relationships (254, 255)]\", 'summary': 'Llama Guard models for AI safety'}\n {'explanation': \"ShieldGemma is an advanced content moderation model available on HuggingFace, designed to enhance safety and reliability in interactions between LLMs and users. This model plays a critical role in ensuring that AI-generated content adheres to safety standards, thereby reducing the risk of harmful outputs. The availability of ShieldGemma on HuggingFace's platform underscores the company's dedication to providing tools that promote responsible AI usage and mitigate risks associated with AI-generated content. [Data: Entities (286); Relationships (256)]\", 'summary': \"ShieldGemma's role in content moderation\"}\n {'explanation': 'SetFit is a tool developed by HuggingFace that allows users to fine-tune models with minimal coding and data requirements. This tool democratizes access to advanced model training techniques, enabling a broader audience to leverage state-of-the-art AI capabilities. By simplifying the fine-tuning process, SetFit empowers developers and researchers to optimize their models effectively, contributing to the overall growth of the AI community. [Data: Entities (363); Relationships (337)]', 'summary': 'SetFit tool for model fine-tuning'}\n {'explanation': \"LangChain is a framework that facilitates the development of applications using language models, and HuggingFace's models can be utilized within this framework. This integration allows developers to create sophisticated applications that leverage the capabilities of HuggingFace's models, enhancing the versatility and applicability of AI technologies. The collaboration between HuggingFace and LangChain exemplifies the community's focus on innovation and the development of practical AI solutions. [Data: Entities (370); Relationships (340)]\", 'summary': 'LangChain framework for application development'}\n {'explanation': \"Optimum is a comprehensive suite of optimization tools developed by HuggingFace, specifically designed to enhance the performance and efficiency of large language models. By implementing advanced strategies such as pruning and model distillation, Optimum aims to facilitate the deployment of LLMs across various hardware platforms. This focus on optimization not only boosts the performance of LLMs but also makes them more accessible and practical for a wide range of applications, reinforcing HuggingFace's position as a leader in the AI community. [Data: Entities (373); Relationships (354)]\", 'summary': 'Optimum suite for model optimization'}]",
         "{\n    \"title\": \"HuggingFace and AI Safety Models\",\n    \"summary\": \"The community centers around HuggingFace, a leading entity in artificial intelligence, particularly in natural language processing. HuggingFace collaborates with various AI safety models such as Llama Guard 2, Llama Guard 3, and ShieldGemma, enhancing the safety and usability of large language models. The interconnectedness of these entities highlights a robust ecosystem focused on responsible AI development and optimization.\",\n    \"findings\": [\n        {\n            \"summary\": \"HuggingFace as a central player in AI development\",\n            \"explanation\": \"HuggingFace is a prominent company recognized for its significant contributions to artificial intelligence, especially in natural language processing (NLP) and machine learning. The company is best known for its Transformers library, which serves as a foundational tool for fine-tuning large language models (LLMs). HuggingFace's commitment to collaboration and safety is evident through its partnerships with various organizations to evaluate and ensure the safety standards of AI models. This central role positions HuggingFace as a key player in the ongoing development of AI technologies, making it a vital entity within this community. [Data: Entities (98); Relationships (73, 193, 254, 255, 256)]\"\n        },\n        {\n            \"summary\": \"Integration of Direct Preference Optimization (DPO)\",\n            \"explanation\": \"Direct Preference Optimization (DPO) is a method supported by HuggingFace to align language models with human preferences. This approach simplifies the process of incorporating human feedback into model training, making it more accessible for developers. The integration of DPO within HuggingFace's libraries enhances the usability of their models, allowing for more effective alignment with user expectations and preferences. This capability is crucial for the development of responsible AI applications that prioritize user experience. [Data: Entities (213); Relationships (193)]\"\n        },\n        {\n            \"summary\": \"Llama Guard models for AI safety\",\n            \"explanation\": \"Llama Guard 2 and Llama Guard 3 are safeguard models built on LLMs, focusing on managing risks in conversational AI applications. These models are evaluated and made accessible through HuggingFace's platform, indicating a strong commitment to AI safety. The introduction of new categories for safety evaluation in Llama Guard 3 reflects the evolving landscape of AI interactions and the need for robust safety measures. By incorporating these models, HuggingFace enhances the reliability of AI applications, ensuring safer interactions between users and AI systems. [Data: Entities (284, 285); Relationships (254, 255)]\"\n        },\n        {\n            \"summary\": \"ShieldGemma's role in content moderation\",\n            \"explanation\": \"ShieldGemma is an advanced content moderation model available on HuggingFace, designed to enhance safety and reliability in interactions between LLMs and users. This model plays a critical role in ensuring that AI-generated content adheres to safety standards, thereby reducing the risk of harmful outputs. The availability of ShieldGemma on HuggingFace's platform underscores the company's dedication to providing tools that promote responsible AI usage and mitigate risks associated with AI-generated content. [Data: Entities (286); Relationships (256)]\"\n        },\n        {\n            \"summary\": \"SetFit tool for model fine-tuning\",\n            \"explanation\": \"SetFit is a tool developed by HuggingFace that allows users to fine-tune models with minimal coding and data requirements. This tool democratizes access to advanced model training techniques, enabling a broader audience to leverage state-of-the-art AI capabilities. By simplifying the fine-tuning process, SetFit empowers developers and researchers to optimize their models effectively, contributing to the overall growth of the AI community. [Data: Entities (363); Relationships (337)]\"\n        },\n        {\n            \"summary\": \"LangChain framework for application development\",\n            \"explanation\": \"LangChain is a framework that facilitates the development of applications using language models, and HuggingFace's models can be utilized within this framework. This integration allows developers to create sophisticated applications that leverage the capabilities of HuggingFace's models, enhancing the versatility and applicability of AI technologies. The collaboration between HuggingFace and LangChain exemplifies the community's focus on innovation and the development of practical AI solutions. [Data: Entities (370); Relationships (340)]\"\n        },\n        {\n            \"summary\": \"Optimum suite for model optimization\",\n            \"explanation\": \"Optimum is a comprehensive suite of optimization tools developed by HuggingFace, specifically designed to enhance the performance and efficiency of large language models. By implementing advanced strategies such as pruning and model distillation, Optimum aims to facilitate the deployment of LLMs across various hardware platforms. This focus on optimization not only boosts the performance of LLMs but also makes them more accessible and practical for a wide range of applications, reinforcing HuggingFace's position as a leader in the AI community. [Data: Entities (373); Relationships (354)]\"\n        }\n    ],\n    \"rating\": 8.5,\n    \"rating_explanation\": \"The impact severity rating is high due to the significant influence of HuggingFace and its associated safety models on the development and deployment of AI technologies.\"\n}",
         "2025-12-14",
         "8"
        ],
        [
         "3",
         "0a4ce9c567044ee5a1fd0ca1563030ba",
         "75",
         "75",
         "2",
         "31",
         "[]",
         "QLoRA and Its Applications in NLP",
         "The community centers around QLoRA, an advanced fine-tuning method for large language models, and its application by a tech company for deploying NLP models on mobile devices. The relationship between QLoRA, the tech company, and the Phi-2 model highlights the significance of efficient model training and deployment in the field of natural language processing.",
         "# QLoRA and Its Applications in NLP\n\nThe community centers around QLoRA, an advanced fine-tuning method for large language models, and its application by a tech company for deploying NLP models on mobile devices. The relationship between QLoRA, the tech company, and the Phi-2 model highlights the significance of efficient model training and deployment in the field of natural language processing.\n\n## QLoRA as a pivotal fine-tuning method\n\nQLoRA is a significant advancement in the fine-tuning of large language models, designed to enhance efficiency while minimizing memory usage. By quantizing weight parameters to a precision of 4 bits, QLoRA reduces the memory footprint required for model training and deployment. This method is particularly beneficial in environments with limited hardware capabilities, allowing practitioners to maintain performance levels comparable to traditional fine-tuning methods. The importance of QLoRA in the community is underscored by its application in various NLP scenarios, making it a cornerstone of modern machine learning practices. [Data: Entities (177)]\n\n## Tech Company's utilization of QLoRA\n\nThe tech company represents a practical application of QLoRA, utilizing this fine-tuning method to deploy advanced NLP models on mobile devices. This relationship illustrates the real-world impact of QLoRA, as it enables the company to leverage quantized large language models effectively. The deployment of these models on mobile devices signifies a shift towards more efficient and accessible NLP solutions, catering to a broader audience. The tech company's innovative approach highlights the potential of QLoRA in enhancing the capabilities of mobile applications. [Data: Entities (324); Relationships (291)]\n\n## Phi-2 model and QLoRA tutorial\n\nThe Phi-2 model is specifically mentioned in the context of QLoRA, as the tutorial explains the fine-tuning process for this model. This relationship emphasizes the educational aspect of QLoRA, providing practitioners with the necessary guidance to implement this advanced technique effectively. The tutorial serves as a valuable resource for those looking to understand the intricacies of fine-tuning with QLoRA, thereby fostering a community of informed users who can apply these methods in their own projects. [Data: Entities (182); Relationships (148)]\n\n## The significance of quantization in NLP\n\nQuantization, as employed by QLoRA, plays a crucial role in the efficiency of NLP models. By reducing the precision of weight parameters, QLoRA allows for significant reductions in memory usage without compromising performance. This is particularly important in the context of deploying models on mobile devices, where computational resources are often limited. The ability to maintain high performance while minimizing resource requirements is a key advantage of QLoRA, making it a vital tool for developers in the NLP field. [Data: Entities (177)]\n\n## Community impact on NLP development\n\nThe community surrounding QLoRA and its applications has a notable impact on the development of natural language processing technologies. By facilitating the deployment of advanced models in resource-constrained environments, QLoRA contributes to the democratization of NLP technologies. This community fosters innovation and collaboration among practitioners, leading to the development of more efficient and accessible NLP solutions. The ongoing advancements in this area are likely to shape the future of NLP, making it an exciting field for researchers and developers alike. [Data: Entities (177, 324, 182); Relationships (291, 148)]",
         "7.5",
         "The impact severity rating is high due to the significant advancements in NLP efficiency and accessibility enabled by QLoRA.",
         "[{'explanation': 'QLoRA is a significant advancement in the fine-tuning of large language models, designed to enhance efficiency while minimizing memory usage. By quantizing weight parameters to a precision of 4 bits, QLoRA reduces the memory footprint required for model training and deployment. This method is particularly beneficial in environments with limited hardware capabilities, allowing practitioners to maintain performance levels comparable to traditional fine-tuning methods. The importance of QLoRA in the community is underscored by its application in various NLP scenarios, making it a cornerstone of modern machine learning practices. [Data: Entities (177)]', 'summary': 'QLoRA as a pivotal fine-tuning method'}\n {'explanation': \"The tech company represents a practical application of QLoRA, utilizing this fine-tuning method to deploy advanced NLP models on mobile devices. This relationship illustrates the real-world impact of QLoRA, as it enables the company to leverage quantized large language models effectively. The deployment of these models on mobile devices signifies a shift towards more efficient and accessible NLP solutions, catering to a broader audience. The tech company's innovative approach highlights the potential of QLoRA in enhancing the capabilities of mobile applications. [Data: Entities (324); Relationships (291)]\", 'summary': \"Tech Company's utilization of QLoRA\"}\n {'explanation': 'The Phi-2 model is specifically mentioned in the context of QLoRA, as the tutorial explains the fine-tuning process for this model. This relationship emphasizes the educational aspect of QLoRA, providing practitioners with the necessary guidance to implement this advanced technique effectively. The tutorial serves as a valuable resource for those looking to understand the intricacies of fine-tuning with QLoRA, thereby fostering a community of informed users who can apply these methods in their own projects. [Data: Entities (182); Relationships (148)]', 'summary': 'Phi-2 model and QLoRA tutorial'}\n {'explanation': 'Quantization, as employed by QLoRA, plays a crucial role in the efficiency of NLP models. By reducing the precision of weight parameters, QLoRA allows for significant reductions in memory usage without compromising performance. This is particularly important in the context of deploying models on mobile devices, where computational resources are often limited. The ability to maintain high performance while minimizing resource requirements is a key advantage of QLoRA, making it a vital tool for developers in the NLP field. [Data: Entities (177)]', 'summary': 'The significance of quantization in NLP'}\n {'explanation': 'The community surrounding QLoRA and its applications has a notable impact on the development of natural language processing technologies. By facilitating the deployment of advanced models in resource-constrained environments, QLoRA contributes to the democratization of NLP technologies. This community fosters innovation and collaboration among practitioners, leading to the development of more efficient and accessible NLP solutions. The ongoing advancements in this area are likely to shape the future of NLP, making it an exciting field for researchers and developers alike. [Data: Entities (177, 324, 182); Relationships (291, 148)]', 'summary': 'Community impact on NLP development'}]",
         "{\n    \"title\": \"QLoRA and Its Applications in NLP\",\n    \"summary\": \"The community centers around QLoRA, an advanced fine-tuning method for large language models, and its application by a tech company for deploying NLP models on mobile devices. The relationship between QLoRA, the tech company, and the Phi-2 model highlights the significance of efficient model training and deployment in the field of natural language processing.\",\n    \"findings\": [\n        {\n            \"summary\": \"QLoRA as a pivotal fine-tuning method\",\n            \"explanation\": \"QLoRA is a significant advancement in the fine-tuning of large language models, designed to enhance efficiency while minimizing memory usage. By quantizing weight parameters to a precision of 4 bits, QLoRA reduces the memory footprint required for model training and deployment. This method is particularly beneficial in environments with limited hardware capabilities, allowing practitioners to maintain performance levels comparable to traditional fine-tuning methods. The importance of QLoRA in the community is underscored by its application in various NLP scenarios, making it a cornerstone of modern machine learning practices. [Data: Entities (177)]\"\n        },\n        {\n            \"summary\": \"Tech Company's utilization of QLoRA\",\n            \"explanation\": \"The tech company represents a practical application of QLoRA, utilizing this fine-tuning method to deploy advanced NLP models on mobile devices. This relationship illustrates the real-world impact of QLoRA, as it enables the company to leverage quantized large language models effectively. The deployment of these models on mobile devices signifies a shift towards more efficient and accessible NLP solutions, catering to a broader audience. The tech company's innovative approach highlights the potential of QLoRA in enhancing the capabilities of mobile applications. [Data: Entities (324); Relationships (291)]\"\n        },\n        {\n            \"summary\": \"Phi-2 model and QLoRA tutorial\",\n            \"explanation\": \"The Phi-2 model is specifically mentioned in the context of QLoRA, as the tutorial explains the fine-tuning process for this model. This relationship emphasizes the educational aspect of QLoRA, providing practitioners with the necessary guidance to implement this advanced technique effectively. The tutorial serves as a valuable resource for those looking to understand the intricacies of fine-tuning with QLoRA, thereby fostering a community of informed users who can apply these methods in their own projects. [Data: Entities (182); Relationships (148)]\"\n        },\n        {\n            \"summary\": \"The significance of quantization in NLP\",\n            \"explanation\": \"Quantization, as employed by QLoRA, plays a crucial role in the efficiency of NLP models. By reducing the precision of weight parameters, QLoRA allows for significant reductions in memory usage without compromising performance. This is particularly important in the context of deploying models on mobile devices, where computational resources are often limited. The ability to maintain high performance while minimizing resource requirements is a key advantage of QLoRA, making it a vital tool for developers in the NLP field. [Data: Entities (177)]\"\n        },\n        {\n            \"summary\": \"Community impact on NLP development\",\n            \"explanation\": \"The community surrounding QLoRA and its applications has a notable impact on the development of natural language processing technologies. By facilitating the deployment of advanced models in resource-constrained environments, QLoRA contributes to the democratization of NLP technologies. This community fosters innovation and collaboration among practitioners, leading to the development of more efficient and accessible NLP solutions. The ongoing advancements in this area are likely to shape the future of NLP, making it an exciting field for researchers and developers alike. [Data: Entities (177, 324, 182); Relationships (291, 148)]\"\n        }\n    ],\n    \"rating\": 7.5,\n    \"rating_explanation\": \"The impact severity rating is high due to the significant advancements in NLP efficiency and accessibility enabled by QLoRA.\"\n}",
         "2025-12-14",
         "3"
        ],
        [
         "4",
         "c7c398031b234d9ca0a0ec4986978cf2",
         "19",
         "19",
         "1",
         "0",
         "[]",
         "PagerDuty and Alerting Systems Community",
         "The community centers around PagerDuty, an incident management tool, and its integration with alerting systems and Slack for effective communication during incidents. The relationships among these entities highlight their collaborative role in managing incidents and ensuring timely notifications.",
         "# PagerDuty and Alerting Systems Community\n\nThe community centers around PagerDuty, an incident management tool, and its integration with alerting systems and Slack for effective communication during incidents. The relationships among these entities highlight their collaborative role in managing incidents and ensuring timely notifications.\n\n## PagerDuty as a central incident management tool\n\nPagerDuty serves as a crucial communication tool for incident management, allowing organizations to respond effectively to incidents. Its integration into alerting systems enhances its functionality, enabling it to notify relevant personnel about significant events or anomalies. This central role in incident management underscores the importance of PagerDuty in maintaining operational continuity and minimizing downtime. [Data: Entities (356); Relationships (331)]\n\n## Integration of Alerting Systems with PagerDuty\n\nThe alerting system is designed to notify users of significant events, and its integration with PagerDuty allows for streamlined incident management. This relationship indicates that alerting systems are not standalone entities but rather work in conjunction with PagerDuty to ensure that incidents are managed efficiently. The combined degree of 4 suggests a strong interdependence, which is critical for organizations that rely on timely alerts to mitigate risks. [Data: Entities (358); Relationships (331)]\n\n## Role of Slack in incident communication\n\nSlack's integration into alerting systems facilitates real-time communication during incidents, allowing teams to collaborate effectively. This relationship highlights the importance of communication tools in incident management, as they enable quick dissemination of information and coordination among team members. The combined degree of 4 indicates that Slack plays a significant role in enhancing the responsiveness of alerting systems. [Data: Entities (360); Relationships (333)]\n\n## Collaboration between Alerting Systems and Slack\n\nThe relationship between alerting systems and Slack demonstrates how technology can enhance incident response capabilities. By integrating Slack, alerting systems can provide immediate notifications to teams, ensuring that all relevant personnel are informed and can act swiftly. This collaboration is essential for organizations that prioritize rapid response to incidents, thereby reducing potential impacts on operations. [Data: Relationships (333)]\n\n## Overall impact of the community on incident management\n\nThe interconnectedness of PagerDuty, alerting systems, and Slack illustrates a robust framework for incident management. This community's structure allows organizations to effectively monitor, alert, and communicate during incidents, which is vital for maintaining service reliability and operational efficiency. The integration of these tools signifies a proactive approach to incident management, which can significantly reduce the severity and duration of incidents. [Data: Relationships (331, 333)]",
         "7.5",
         "The impact severity rating is high due to the critical role these entities play in managing incidents and ensuring effective communication during emergencies.",
         "[{'explanation': 'PagerDuty serves as a crucial communication tool for incident management, allowing organizations to respond effectively to incidents. Its integration into alerting systems enhances its functionality, enabling it to notify relevant personnel about significant events or anomalies. This central role in incident management underscores the importance of PagerDuty in maintaining operational continuity and minimizing downtime. [Data: Entities (356); Relationships (331)]', 'summary': 'PagerDuty as a central incident management tool'}\n {'explanation': 'The alerting system is designed to notify users of significant events, and its integration with PagerDuty allows for streamlined incident management. This relationship indicates that alerting systems are not standalone entities but rather work in conjunction with PagerDuty to ensure that incidents are managed efficiently. The combined degree of 4 suggests a strong interdependence, which is critical for organizations that rely on timely alerts to mitigate risks. [Data: Entities (358); Relationships (331)]', 'summary': 'Integration of Alerting Systems with PagerDuty'}\n {'explanation': \"Slack's integration into alerting systems facilitates real-time communication during incidents, allowing teams to collaborate effectively. This relationship highlights the importance of communication tools in incident management, as they enable quick dissemination of information and coordination among team members. The combined degree of 4 indicates that Slack plays a significant role in enhancing the responsiveness of alerting systems. [Data: Entities (360); Relationships (333)]\", 'summary': 'Role of Slack in incident communication'}\n {'explanation': 'The relationship between alerting systems and Slack demonstrates how technology can enhance incident response capabilities. By integrating Slack, alerting systems can provide immediate notifications to teams, ensuring that all relevant personnel are informed and can act swiftly. This collaboration is essential for organizations that prioritize rapid response to incidents, thereby reducing potential impacts on operations. [Data: Relationships (333)]', 'summary': 'Collaboration between Alerting Systems and Slack'}\n {'explanation': \"The interconnectedness of PagerDuty, alerting systems, and Slack illustrates a robust framework for incident management. This community's structure allows organizations to effectively monitor, alert, and communicate during incidents, which is vital for maintaining service reliability and operational efficiency. The integration of these tools signifies a proactive approach to incident management, which can significantly reduce the severity and duration of incidents. [Data: Relationships (331, 333)]\", 'summary': 'Overall impact of the community on incident management'}]",
         "{\n    \"title\": \"PagerDuty and Alerting Systems Community\",\n    \"summary\": \"The community centers around PagerDuty, an incident management tool, and its integration with alerting systems and Slack for effective communication during incidents. The relationships among these entities highlight their collaborative role in managing incidents and ensuring timely notifications.\",\n    \"findings\": [\n        {\n            \"summary\": \"PagerDuty as a central incident management tool\",\n            \"explanation\": \"PagerDuty serves as a crucial communication tool for incident management, allowing organizations to respond effectively to incidents. Its integration into alerting systems enhances its functionality, enabling it to notify relevant personnel about significant events or anomalies. This central role in incident management underscores the importance of PagerDuty in maintaining operational continuity and minimizing downtime. [Data: Entities (356); Relationships (331)]\"\n        },\n        {\n            \"summary\": \"Integration of Alerting Systems with PagerDuty\",\n            \"explanation\": \"The alerting system is designed to notify users of significant events, and its integration with PagerDuty allows for streamlined incident management. This relationship indicates that alerting systems are not standalone entities but rather work in conjunction with PagerDuty to ensure that incidents are managed efficiently. The combined degree of 4 suggests a strong interdependence, which is critical for organizations that rely on timely alerts to mitigate risks. [Data: Entities (358); Relationships (331)]\"\n        },\n        {\n            \"summary\": \"Role of Slack in incident communication\",\n            \"explanation\": \"Slack's integration into alerting systems facilitates real-time communication during incidents, allowing teams to collaborate effectively. This relationship highlights the importance of communication tools in incident management, as they enable quick dissemination of information and coordination among team members. The combined degree of 4 indicates that Slack plays a significant role in enhancing the responsiveness of alerting systems. [Data: Entities (360); Relationships (333)]\"\n        },\n        {\n            \"summary\": \"Collaboration between Alerting Systems and Slack\",\n            \"explanation\": \"The relationship between alerting systems and Slack demonstrates how technology can enhance incident response capabilities. By integrating Slack, alerting systems can provide immediate notifications to teams, ensuring that all relevant personnel are informed and can act swiftly. This collaboration is essential for organizations that prioritize rapid response to incidents, thereby reducing potential impacts on operations. [Data: Relationships (333)]\"\n        },\n        {\n            \"summary\": \"Overall impact of the community on incident management\",\n            \"explanation\": \"The interconnectedness of PagerDuty, alerting systems, and Slack illustrates a robust framework for incident management. This community's structure allows organizations to effectively monitor, alert, and communicate during incidents, which is vital for maintaining service reliability and operational efficiency. The integration of these tools signifies a proactive approach to incident management, which can significantly reduce the severity and duration of incidents. [Data: Relationships (331, 333)]\"\n        }\n    ],\n    \"rating\": 7.5,\n    \"rating_explanation\": \"The impact severity rating is high due to the critical role these entities play in managing incidents and ensuring effective communication during emergencies.\"\n}",
         "2025-12-14",
         "3"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>parent</th>\n",
       "      <th>children</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>full_content</th>\n",
       "      <th>rank</th>\n",
       "      <th>rating_explanation</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>period</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>082bb8c74db541a48fb16acfe85e3c67</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>[]</td>\n",
       "      <td>PyTorch and Large Language Models Community</td>\n",
       "      <td>The community centers around PyTorch, a leadin...</td>\n",
       "      <td># PyTorch and Large Language Models Community\\...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'PyTorch is a widely used ope...</td>\n",
       "      <td>{\\n    \"title\": \"PyTorch and Large Language Mo...</td>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf5a75cc720e4ed29547f2c43dd86ea0</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>[]</td>\n",
       "      <td>Validation Loops and Trends in Validation Metrics</td>\n",
       "      <td>The community focuses on the relationship betw...</td>\n",
       "      <td># Validation Loops and Trends in Validation Me...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'Validation Loops are critica...</td>\n",
       "      <td>{\\n    \"title\": \"Validation Loops and Trends i...</td>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a91f21e9e4ed4ab5a0e927b72f1b10b2</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>[]</td>\n",
       "      <td>HuggingFace and AI Safety Models</td>\n",
       "      <td>The community centers around HuggingFace, a le...</td>\n",
       "      <td># HuggingFace and AI Safety Models\\n\\nThe comm...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'HuggingFace is a prominent c...</td>\n",
       "      <td>{\\n    \"title\": \"HuggingFace and AI Safety Mod...</td>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a4ce9c567044ee5a1fd0ca1563030ba</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>[]</td>\n",
       "      <td>QLoRA and Its Applications in NLP</td>\n",
       "      <td>The community centers around QLoRA, an advance...</td>\n",
       "      <td># QLoRA and Its Applications in NLP\\n\\nThe com...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'QLoRA is a significant advan...</td>\n",
       "      <td>{\\n    \"title\": \"QLoRA and Its Applications in...</td>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c7c398031b234d9ca0a0ec4986978cf2</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>PagerDuty and Alerting Systems Community</td>\n",
       "      <td>The community centers around PagerDuty, an inc...</td>\n",
       "      <td># PagerDuty and Alerting Systems Community\\n\\n...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'PagerDuty serves as a crucia...</td>\n",
       "      <td>{\\n    \"title\": \"PagerDuty and Alerting System...</td>\n",
       "      <td>2025-12-14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  human_readable_id  community  level  \\\n",
       "0  082bb8c74db541a48fb16acfe85e3c67                 72         72      2   \n",
       "1  cf5a75cc720e4ed29547f2c43dd86ea0                 73         73      2   \n",
       "2  a91f21e9e4ed4ab5a0e927b72f1b10b2                 74         74      2   \n",
       "3  0a4ce9c567044ee5a1fd0ca1563030ba                 75         75      2   \n",
       "4  c7c398031b234d9ca0a0ec4986978cf2                 19         19      1   \n",
       "\n",
       "   parent children                                              title  \\\n",
       "0      21       []        PyTorch and Large Language Models Community   \n",
       "1      21       []  Validation Loops and Trends in Validation Metrics   \n",
       "2      31       []                   HuggingFace and AI Safety Models   \n",
       "3      31       []                  QLoRA and Its Applications in NLP   \n",
       "4       0       []           PagerDuty and Alerting Systems Community   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The community centers around PyTorch, a leadin...   \n",
       "1  The community focuses on the relationship betw...   \n",
       "2  The community centers around HuggingFace, a le...   \n",
       "3  The community centers around QLoRA, an advance...   \n",
       "4  The community centers around PagerDuty, an inc...   \n",
       "\n",
       "                                        full_content  rank  \\\n",
       "0  # PyTorch and Large Language Models Community\\...   8.0   \n",
       "1  # Validation Loops and Trends in Validation Me...   7.0   \n",
       "2  # HuggingFace and AI Safety Models\\n\\nThe comm...   8.5   \n",
       "3  # QLoRA and Its Applications in NLP\\n\\nThe com...   7.5   \n",
       "4  # PagerDuty and Alerting Systems Community\\n\\n...   7.5   \n",
       "\n",
       "                                  rating_explanation  \\\n",
       "0  The impact severity rating is high due to the ...   \n",
       "1  The impact severity rating is high due to the ...   \n",
       "2  The impact severity rating is high due to the ...   \n",
       "3  The impact severity rating is high due to the ...   \n",
       "4  The impact severity rating is high due to the ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'PyTorch is a widely used ope...   \n",
       "1  [{'explanation': 'Validation Loops are critica...   \n",
       "2  [{'explanation': 'HuggingFace is a prominent c...   \n",
       "3  [{'explanation': 'QLoRA is a significant advan...   \n",
       "4  [{'explanation': 'PagerDuty serves as a crucia...   \n",
       "\n",
       "                                   full_content_json      period  size  \n",
       "0  {\\n    \"title\": \"PyTorch and Large Language Mo...  2025-12-14    26  \n",
       "1  {\\n    \"title\": \"Validation Loops and Trends i...  2025-12-14     2  \n",
       "2  {\\n    \"title\": \"HuggingFace and AI Safety Mod...  2025-12-14     8  \n",
       "3  {\\n    \"title\": \"QLoRA and Its Applications in...  2025-12-14     3  \n",
       "4  {\\n    \"title\": \"PagerDuty and Alerting System...  2025-12-14     3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_reports = pd.read_parquet('./paper/output/community_reports.parquet')\n",
    "\n",
    "community_reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca51c33-92db-47c1-9f00-0a8e934332fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PyTorch and Large Language Models Community\n",
      "\n",
      "The community centers around PyTorch, a leading machine learning library, and its relationship with Large Language Models (LLMs) and associated entities like Hugging Face and Meta. These entities collaborate to advance the development and ethical deployment of AI technologies, particularly in natural language processing.\n",
      "\n",
      "## PyTorch as a foundational framework for LLMs\n",
      "\n",
      "PyTorch is a widely used open-source machine learning library that plays a crucial role in the development and training of Large Language Models (LLMs). It provides a flexible platform that supports both the initialization and fine-tuning of these models, making it integral to the field of natural language processing. The framework's capabilities enable researchers and developers to create sophisticated LLMs that can understand and generate human-like text, which is essential for various applications, from chatbots to advanced AI systems. This foundational role underscores PyTorch's significance in the ongoing evolution of artificial intelligence technologies [Data: Entities (99); Relationships (76)].\n",
      "\n",
      "## Hugging Face's contribution to LLM accessibility\n",
      "\n",
      "Hugging Face is a prominent company that specializes in natural language processing and provides tools and libraries specifically designed for working with LLMs. Its comprehensive library facilitates the use of pre-trained models and tokenizers, making it easier for developers and researchers to implement advanced NLP techniques in their projects. Additionally, Hugging Face offers an Inference API that simplifies the deployment and management of LLMs, allowing users to leverage these powerful models without needing extensive expertise in machine learning. This commitment to accessibility and usability significantly contributes to the advancement of NLP technologies [Data: Entities (110); Relationships (93)].\n",
      "\n",
      "## Ethical considerations in LLM deployment\n",
      "\n",
      "Ethical considerations are paramount in the deployment of LLMs, as they address critical issues related to the responsible and fair use of AI technologies. This includes the potential for biases in training data, which can lead to models that inadvertently amplify these biases. The community emphasizes the importance of scrutinizing data used in model training to mitigate harmful or biased content. Furthermore, ethical considerations extend to the moral implications of deploying AI technologies, ensuring that AI systems are used fairly and justly, thereby fostering trust and accountability in AI applications [Data: Entities (97); Relationships (302)].\n",
      "\n",
      "## Meta's advancements in LLM technology\n",
      "\n",
      "Meta is recognized for its significant contributions to both social media and artificial intelligence, particularly through its LLaMA project, which focuses on adapting LLMs for various tasks. This initiative reflects Meta's commitment to advancing AI and machine learning, showcasing its expertise in developing sophisticated algorithms that can understand and generate human-like text. The integration of advanced AI technologies into Meta's social media platforms enhances user experience and engagement, demonstrating the company's dual focus on social connectivity and technological innovation [Data: Entities (398); Relationships (446)].\n",
      "\n",
      "## The role of evaluation metrics in LLM performance\n",
      "\n",
      "Evaluation metrics are essential for assessing the performance of LLMs and ensuring quality outputs. Metrics such as perplexity, factuality, and completeness are critical for evaluating how well LLMs predict the next word in a sequence, the accuracy of their outputs, and whether they fully address user queries based on the provided context. These metrics help identify low-quality outputs and ensure that LLMs produce coherent and contextually relevant text, which is vital for applications where misinformation could have serious consequences [Data: Entities (242, 243, 246); Relationships (221, 222, 226)].\n"
     ]
    }
   ],
   "source": [
    "print(community_reports[\"full_content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c3e76b-e59f-4577-b1a0-3b07f0ebb499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The community centers around PyTorch, a leading machine learning library, and its relationship with Large Language Models (LLMs) and associated entities like Hugging Face and Meta. These entities collaborate to advance the development and ethical deployment of AI technologies, particularly in natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(community_reports[\"summary\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5243d1-e1d2-4525-ab3c-e1457db2eea7",
   "metadata": {},
   "source": [
    "### The Final Graph!\n",
    "\n",
    "<img src=\"./media/ghraphrag_viz.svg\" width=800>\n",
    "\n",
    "*[Full Size PDF](./ghraphrag_viz.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bf775-ee3b-4d4a-a973-317f1681b8af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Retrieval\n",
    "\n",
    "<img src=\"./media/kg_retrieval.png\" width=600>\n",
    "\n",
    "*[Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/pdf/2306.08302)*\n",
    "\n",
    "With our knowledge graph constructed, and hierarchichal communities delineated, we can now perform multiple types of search that can both take advantage of the graph structure, and multiple levels of specificity across our communities. Specifically:\n",
    "\n",
    "1. **Global Search**: Uses the LLM Generated community reports from a specified level of the graph's community hierarchy as context data to generate response.\n",
    "2. **Local Search**: Combines structured data from the knowledge graph with unstructured data from the input document(s) to augment the LLM context with relevant entity information.\n",
    "3. **Drift Search**: Dynamic Reasoning and Inference with Flexible Traversal, an approach to local search queries by including community information in the search process, thus combining global and local search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a5823-048b-4a54-9da5-51305b8a1c7a",
   "metadata": {},
   "source": [
    "**GraphRAG Retrieval Function**\n",
    "\n",
    "*Note: Wrapping the [GraphRAG CLI tool](https://microsoft.github.io/graphrag/cli/) as a function here instead of using their [library](https://microsoft.github.io/graphrag/examples_notebooks/api_overview/) for an easier example. As such, notebook needs to be running in the same GraphRAG environment/kernal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae01778e-518f-42fe-929e-dd2ef63a8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from typing import Optional\n",
    "\n",
    "def query_graphrag(\n",
    "    query: str,\n",
    "    method: str = \"global\",\n",
    "    root_path: str = \"./paper\",\n",
    "    timeout: Optional[int] = None,\n",
    "    community_level: int = 2,\n",
    "    dynamic_community_selection: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Execute a GraphRAG query using the CLI tool.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query string to process\n",
    "        method (str): Query method (e.g., \"global\", \"local\", or \"drift\")\n",
    "        root_path (str): Path to the root directory\n",
    "        timeout (int, optional): Timeout in seconds for the command\n",
    "        community_level (int): The community level in the Leiden community hierarchy (default: 2)\n",
    "        dynamic_community_selection (bool): Whether to use global search with dynamic community selection (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: The output from GraphRAG\n",
    "        \n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the command fails\n",
    "        subprocess.TimeoutExpired: If the command times out\n",
    "        ValueError: If community_level is negative\n",
    "    \"\"\"\n",
    "    # Validate community level\n",
    "    if community_level < 0:\n",
    "        raise ValueError(\"Community level must be non-negative\")\n",
    "    \n",
    "    # Construct the base command\n",
    "    command = [\n",
    "        'graphrag', 'query',\n",
    "        '--root', root_path,\n",
    "        '--method', method,\n",
    "        '--query', query,\n",
    "        '--community-level', str(community_level)\n",
    "    ]\n",
    "    \n",
    "    # Add dynamic community selection flag if enabled\n",
    "    if dynamic_community_selection:\n",
    "        command.append('--dynamic-community-selection')\n",
    "    \n",
    "    try:\n",
    "        # Execute the command and capture output\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        result.check_returncode()\n",
    "        \n",
    "        return result.stdout.strip()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        error_message = f\"Command failed with exit code {e.returncode}\\nError: {e.stderr}\"\n",
    "        raise subprocess.CalledProcessError(\n",
    "            e.returncode,\n",
    "            e.cmd,\n",
    "            output=e.output,\n",
    "            stderr=error_message\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02da6ea-e0ad-41d4-982a-f9058184add0",
   "metadata": {},
   "source": [
    "### Local Search\n",
    "\n",
    "<img src=\"./media/local_search.png\" width=900>\n",
    "\n",
    "The GraphRAG approach to local search is the most similar to regular semantic RAG search. It combines structured data from the knowledge graph with unstructured data from the input documents to augment the LLM context with relevant entity information. In essence, we are going to first search for relevant entities to the query using semantic search. These become the entry points on our graph that we can now traverse. Starting at these points, we look at connected chunks of text, community reports, other entities, and relationships between them. All of the data retrieved is filtered and ranked to fit into a pre-defined context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad477f20-1019-4da6-a70a-9cc9e362fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"local\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ae62b",
   "metadata": {},
   "source": [
    "Choosing between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches involves several considerations that align with the specific needs and constraints of a company. Each method has its strengths and weaknesses, making the decision context-dependent.\n",
    "\n",
    "### Understanding the Options\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a method that enhances the capabilities of language models by integrating real-time data retrieval into the generation process. This approach is particularly beneficial for applications requiring up-to-date information, as it allows models to generate contextually relevant responses based on current data. RAG is advantageous when the goal is to provide accurate and timely responses without the need for extensive model retraining [Data: Sources (2)].\n",
    "\n",
    "**Fine-tuning**, on the other hand, involves adapting a pre-trained model to specific tasks by training it further on a smaller, task-specific dataset. This method is essential for improving model performance in specialized applications, such as Automatic Speech Recognition (ASR) or natural language processing (NLP) tasks. Fine-tuning is particularly effective when there is ample domain-specific data available, allowing the model to learn the nuances of the target domain [Data: Reports (6), Entities (32)].\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** techniques, such as Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA), focus on optimizing the fine-tuning process by reducing the number of parameters that need to be adjusted. These methods are designed to enhance efficiency, making them suitable for scenarios where computational resources are limited. PEFT approaches are particularly valuable when organizations need to fine-tune models without incurring high costs or requiring extensive computational power [Data: Reports (5), Entities (49)].\n",
    "\n",
    "### Key Considerations for Decision-Making\n",
    "\n",
    "1. **Data Availability**: If a company has access to a large volume of labeled data specific to its domain, fine-tuning may be the best option. However, if data is scarce or constantly changing, RAG could provide a more flexible solution by leveraging real-time data retrieval.\n",
    "\n",
    "2. **Resource Constraints**: Companies with limited computational resources may benefit from PEFT techniques, which allow for effective model adaptation without the need for extensive retraining. Methods like LoRA and QLoRA are designed to minimize memory usage and computational load, making them ideal for environments with hardware limitations [Data: Reports (5), Entities (180)].\n",
    "\n",
    "3. **Application Requirements**: The choice may also depend on the specific application requirements. For instance, if the goal is to maintain up-to-date responses in a dynamic environment, RAG would be preferable. Conversely, if the focus is on achieving high accuracy in a specific task, fine-tuning or PEFT methods may be more appropriate.\n",
    "\n",
    "4. **Performance vs. Efficiency**: Companies must weigh the trade-offs between performance and efficiency. While fine-tuning can lead to superior performance in specialized tasks, PEFT methods offer a more efficient approach that can still achieve competitive results without the overhead of full model retraining [Data: Reports (6), Entities (32)].\n",
    "\n",
    "5. **Long-term Maintenance**: Consideration of how the chosen method will affect long-term maintenance and scalability is crucial. RAG systems can provide ongoing adaptability to new data, while fine-tuned models may require periodic retraining to maintain performance as data evolves.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In summary, the decision between RAG, fine-tuning, and PEFT approaches should be guided by the specific needs of the organization, including data availability, resource constraints, application requirements, and long-term maintenance considerations. By carefully evaluating these factors, companies can select the most suitable method to enhance their AI capabilities effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4227a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b8368cd-25bb-4460-802f-8ed76a39bb2d",
   "metadata": {},
   "source": [
    "### Global Search\n",
    "\n",
    "<img src=\"./media/global_search.png\" width=1000>\n",
    "\n",
    "Through the semantic clustering of communities during the indexxing process outlined above we created community reports as summaries of high level themes across these groupings. Having this community summary data at various levels allows us to do something that traditional RAG performs poorly at, answering queries about broad themes and ideas across our unstructured data.\n",
    "\n",
    "To capture as much broad information as possible in an efficient manner, GraphRAG implements a [map reduce](https://en.wikipedia.org/wiki/MapReduce) approach. Given a query, relevant community node reports at a specific hierarchical level are retrieved. These are shuffled and chunked, where each chunk is used to generate a list of points that each have their own \"importance score\". These intermediate points are ranked and filtered, attempting to maintain the most important points. These become the aggregate intermediary response, which is passed to the LLM as the context for the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b59560-c02f-4c2e-a969-8042caef03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"global\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c8529",
   "metadata": {},
   "source": [
    "# Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
    "\n",
    "When a company is faced with the decision of selecting between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several critical factors must be considered. Each method has its unique advantages and trade-offs, which can significantly impact the performance and efficiency of AI applications.\n",
    "\n",
    "## Understanding the Approaches\n",
    "\n",
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG is particularly advantageous for applications that require real-time data integration and contextually relevant responses. It enhances language models by incorporating external data, which can significantly improve response accuracy and relevance. This method is ideal for scenarios where up-to-date information is critical, such as customer support or content generation [Data: Reports (7, 55, +more)].\n",
    "\n",
    "### Fine-Tuning\n",
    "\n",
    "Fine-tuning is essential for adapting pre-trained models to specific tasks, allowing for the incorporation of domain-specific knowledge. This approach is most effective when a company has a well-defined task and sufficient labeled data to train the model effectively. Fine-tuning typically requires more extensive computational resources and time, as it involves retraining model weights on a specific dataset [Data: Reports (36)].\n",
    "\n",
    "### Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "PEFT approaches, such as Low-Rank Adaptation (LoRA) and Odds-Ratio Preference Optimization (ORPO), provide a way to align model outputs with desired responses without extensive retraining. These methods are particularly beneficial in environments with limited computational resources, allowing for effective deployment of models without incurring high costs. PEFT techniques focus on optimizing model performance while using fewer parameters, making them suitable for scenarios where rapid deployment is necessary [Data: Reports (10, 30)].\n",
    "\n",
    "## Key Considerations for Decision-Making\n",
    "\n",
    "### Use Case and Requirements\n",
    "\n",
    "The specific use case and requirements of the application shall be the primary drivers in the decision-making process. Companies must evaluate whether they need real-time data integration (favoring RAG), domain-specific adaptation (favoring fine-tuning), or efficient model adaptation with limited resources (favoring PEFT) [Data: Reports (5, 21, 24, 32, 70)].\n",
    "\n",
    "### Data Availability\n",
    "\n",
    "The nature and availability of data are crucial factors. Fine-tuning is most effective when there is a substantial amount of labeled data available for training. In contrast, RAG can leverage existing knowledge bases, which may reduce the need for extensive retraining but requires a robust retrieval mechanism [Data: Reports (5, 21, 24, 32, 70)].\n",
    "\n",
    "### Computational Resources\n",
    "\n",
    "Companies must assess their computational resources and operational constraints. Fine-tuning large models typically requires significant computational power, which may not be feasible for all organizations. PEFT methods, on the other hand, allow for effective model adaptation without extensive retraining, making them more accessible for organizations with limited resources [Data: Reports (6, 10)].\n",
    "\n",
    "### Performance vs. Resource Efficiency\n",
    "\n",
    "The balance between performance needs and resource efficiency is another critical consideration. Fine-tuning can lead to high-performing models tailored to specific tasks, but it may also result in overfitting if not managed properly. RAG can provide up-to-date responses and reduce inaccuracies, but it requires a robust data retrieval system to function effectively. PEFT approaches aim to balance performance and efficiency, making them attractive for organizations looking to optimize their AI capabilities without incurring high costs [Data: Reports (10, 52)].\n",
    "\n",
    "### Scalability and Maintainability\n",
    "\n",
    "Scalability and maintainability of the chosen approach shall also be evaluated. RAG systems can be more complex to maintain due to the need for an effective retrieval system, while fine-tuned models may require ongoing updates as new data becomes available. PEFT approaches can offer a more modular solution, allowing for easier updates and adaptations as new tasks arise [Data: Reports (5, 21, 24, 32, 70)].\n",
    "\n",
    "### Technical Expertise\n",
    "\n",
    "The organization's technical expertise and existing infrastructure may influence the choice of approach. Companies with strong data science teams may prefer fine-tuning, while those looking for quick deployment with less technical overhead might opt for RAG or PEFT methods [Data: Reports (12, 36, 30)].\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ultimately, the decision between RAG, fine-tuning, and PEFT approaches shall be guided by a thorough analysis of the company's goals, data availability, and resource constraints. Companies may benefit from testing different methods to assess their effectiveness in meeting specific performance metrics and user satisfaction. By carefully considering these factors, organizations can select the most suitable approach for their AI applications, ensuring optimal performance and resource utilization [Data: Reports (12, 36, 30)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1d327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35d9948-ba7e-49af-ac49-62b835d62174",
   "metadata": {},
   "source": [
    "### DRIFT Search\n",
    "\n",
    "<img src=\"./media/drift_search.png\" width=1000>\n",
    "\n",
    "[Dynamic Reasoning and Inference with Flexible Traversal](https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/), or DRIFT, is a novel GraphRAG concept introduced by Microsoft as an approach to local search queries that include community information in the search process.\n",
    "\n",
    "The user's query is initially processed through [Hypothetical Document Embedding (HyDE)](https://arxiv.org/pdf/2212.10496), which creates a hypothetical document similar to those found in the graph already, but using the user's topic query. This document is embedded and used for semantic retrieval of the top-k relevant community reports. From these matches, we generate an initial answer along with several follow-up questions as a lightweight version of global search. They refer to this as the primer.\n",
    "\n",
    "Once this primer phase is complete, we execute local searches for each follow-up question generated. Each local search produces both intermediate answers and new follow-up questions, creating a refinement loop. This loop runs for two iterations (noted future research planned to develop reward functions for smarter termination). An important note that makes these local searches unique is that they are informed by both community-level knowledge and detailed entity/relationship data. This allows the DRIFT process to find relevant information even when the initial query diverges from the indexing persona, and it can adapt its approach based on emerging information during the search.\n",
    "\n",
    "The final output is structured as a hierarchy of questions and answers, ranked by their relevance to the original query. Map reduce is used again with an equal weighting on all intermediate answers, then passed to the language model for a final response. DRIFT cleverly combines global and local search with guided exploration to provide both broad context and specific details in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcac67-ab84-4dd7-bf7e-cf4b2a4e7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"drift\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa8fe6",
   "metadata": {},
   "source": [
    "# Choosing Between RAG, Fine-Tuning, and PEFT Approaches                                                                                                                                                                                                           \n",
    "\n",
    "When a company is faced with the decision of selecting between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several critical factors come into play. Each method has its unique strengths and applications, making the choice dependent on specific project requirements, resource availability, and desired outcomes.\n",
    "\n",
    "## 1. Understanding the Methods\n",
    "\n",
    "### Fine-Tuning\n",
    "Fine-tuning involves taking a pre-trained model and training it further on a smaller, task-specific dataset. This process allows the model to adapt its parameters to better suit specific tasks, leading to improved performance in areas such as text classification, sentiment analysis, and question-answering. Companies may opt for fine-tuning when they have access to substantial computational resources and a well-defined dataset that aligns with their objectives [Data: Sources (1, 2, 3)].\n",
    "\n",
    "### Retrieval-Augmented Generation (RAG)\n",
    "RAG enhances language models by integrating external data into the response generation process. This method is particularly effective for tasks requiring up-to-date information or when the dataset is too large to fine-tune effectively. Companies may choose RAG when they need to ensure that their models can access and utilize external knowledge dynamically, which is crucial for applications like chatbots or question-answering systems that require real-time data [Data: Sources (1, 2, 3)].\n",
    "\n",
    "### Parameter-Efficient Fine-Tuning (PEFT)\n",
    "PEFT techniques, such as Low-Rank Adaptation (LoRA) and Quantised LoRA (QLoRA), focus on optimizing the fine-tuning process by adjusting fewer parameters. This method is advantageous for organizations with limited computational resources, as it reduces the memory and processing power required for training. Companies may prefer PEFT when they aim to achieve efficient model adaptation while minimizing costs [Data: Sources (1, 2, 3)].\n",
    "\n",
    "## 2. Key Considerations for Decision-Making\n",
    "\n",
    "### Task Requirements\n",
    "The specific requirements of the task at hand play a significant role in determining the fine-tuning approach. For instance, tasks that involve nuanced language understanding may necessitate more extensive fine-tuning, while those requiring real-time data integration may benefit more from RAG [Data: Sources (1, 2)].\n",
    "\n",
    "### Data Availability\n",
    "The amount and quality of available data are crucial. Fine-tuning typically requires a smaller, task-specific dataset, while RAG can leverage existing data without extensive retraining. If data is scarce, PEFT methods can be advantageous, as they allow for effective adaptation with fewer parameters being updated [Data: Sources (1, 2)].\n",
    "\n",
    "### Computational Resources\n",
    "The computational resources available for fine-tuning are a significant factor. Full fine-tuning of large models can be resource-intensive, requiring substantial GPU or TPU capabilities. In scenarios where resources are limited, PEFT methods can provide a more efficient alternative by updating only a subset of model parameters [Data: Sources (1, 2)].\n",
    "\n",
    "### Performance Outcomes\n",
    "Ultimately, the choice between RAG, fine-tuning, and PEFT approaches depends on the specific needs of the organization, including the desired accuracy, the nature of the tasks, and the importance of real-time data integration. Companies must weigh these factors carefully to select the most suitable approach for their AI applications, ensuring that they achieve the best possible outcomes in terms of performance and user satisfaction [Data: Sources (1, 2)].\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In summary, the decision-making process for choosing between RAG, fine-tuning, and PEFT approaches involves a careful evaluation of task requirements, data availability, computational resources, and desired performance outcomes. By understanding the strengths and limitations of each method, organizations can make informed choices that align with their strategic goals in AI development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ebe0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2e04a5-aa55-4955-b2c0-b86dd462f549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing to Regular Vector Database Retrieval\n",
    "\n",
    "<img src=\"./media/basic_retrieval.png\" width=600>\n",
    " \n",
    "To give some comparison, let's look back at traditional chunking, embedding, and similarity retrieval RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12fa20-81d9-43cc-a641-6e8ba7cb5e9b",
   "metadata": {},
   "source": [
    "**Instantiate our Database**\n",
    "\n",
    "For this we'll be using [ChromaDB](https://www.trychroma.com) with the same chunks as were loaded into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "688068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a361a631-4e98-47d5-9b2d-48810c2eab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./notebook/chromadb\")\n",
    "paper_collection = chroma_client.get_or_create_collection(name=\"paper_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a160165-6ed9-4ac9-be3b-db5bff88beb4",
   "metadata": {},
   "source": [
    "**Embed Chunks Into Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24350f0f-32ef-41ea-9d7c-5e970ca172f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ts75080/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:06<00:00, 12.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for text in texts:\n",
    "    paper_collection.add(\n",
    "        documents=[text],\n",
    "        ids=f\"chunk_{i}\"\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3961f0-bdce-4d1d-8d49-86355bb7d505",
   "metadata": {},
   "source": [
    "**Retrieval Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "841f2276-fc27-4008-9e63-6ec9d4db66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_retrieval(query, num_results=5):\n",
    "    results = paper_collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=num_results\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bb61e-5d36-47fc-b2ec-6557293f5ef4",
   "metadata": {},
   "source": [
    "**RAG Prompt & Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "593a23ff-c34c-47b4-b6d8-c2ca59fad69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"\n",
    "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
    "\n",
    "If you don't know the answer, just say so. Do not make anything up.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "Context: {retrieved_docs}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db1554f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_3', 'chunk_4', 'chunk_2', 'chunk_15', 'chunk_23']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"'s decision-making process. In that case, RAG systems offer insight that is typically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation alongside example use cases.\\n\\n<!-- missing-text -->\\n\\n1.10 Objectives of the Report\\n\\n1.10.1 Goals and Scope\\n\\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs. This involves exploring theoretical foundations, practical implementation strategies, and challenges. The report examines various fine-tuning methodologies, their applications, and recent advancements.\\n\\n1.10.2 Key Questions and Issues Addressed\\n\\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational insights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from pre-training, and emphasises its role in adapting models for specific tasks. Key objectives include enhancing model performance for targeted applications and domains.\\n\\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual representations and detailed stage explanations. It covers practical implementation strategies, including model initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation methods, deployment challenges, and recent advancements are also explored.\\n\\n1.10.3 Overview of the Report Structure\\n\\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters include an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation metrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight ongoing research challenges, and provide insights for researchers and practitioners.\\n\\nChapter 2\\n\\nSeven Stage Fine-Tuning Pipeline for LLM\\n\\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct stages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal performance. These stages encompass everything from initial dataset preparation to the final deployment and maintenance of the fine-tuned model. By following these stages systematically, the model is refined and tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and contextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance.\\n\\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages from dataset preparation to monitoring and maintenance.\\n\\n2.1 Stage 1: Dataset Preparation\\n\\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks by updating its parameters using a new dataset. This involves cleaning and formatting the dataset to match the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is composed of &lt; input output , &gt; pairs, demonstrating the desired behaviour for the model. For example, in instruction tuning, the dataset may look like:\\n\\n###Human: $&lt;Input Query&gt;$ ###Assistant: $&lt;Generated Output&gt;$\\n\\nHere, the 'Input Query' is what the user asks, and the 'Generated Output' is the model's response. The structure and style of these pairs can be adjusted based on the specific needs of the task.\\n\\n2.2 Stage 2: Model Initialisation\\n\\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM before training or deploying it. This step is crucial for ensuring the model performs optimally, trains efficiently, and avoids issues such as vanishing or exploding gradients.\\n\\n2.3 Stage 3: Training Environment Setup\\n\\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure to adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the model's architecture and hyperparameters, and running training iterations to adjust the model's weights and biases. The aim is to enhance the LLM's performance in generating accurate and contextually appropriate outputs tailored to specific applications, like content creation, translation, or sentiment analysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\\n\\n<!-- missing-text -->\\n\\nFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the seven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, FineTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays a crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance throughout its lifecycle.\\n\\n2.4 Stage 4: Partial or Full Fine-Tuning\\n\\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning updates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half fine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter layers, can be employed to partially fine-tune the model. This method attaches additional layers to the pre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges related to computational efficiency, overfitting, and optimisation.\\n\\n2.5 Stage 5: Evaluation and Validation\\n\\nEvaluation and validation involve assessing the fine-tuned LLM's performance on unseen data to ensure it generalises\",\n",
       "   \"PEFT) approaches, such as using adapter layers, can be employed to partially fine-tune the model. This method attaches additional layers to the pre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges related to computational efficiency, overfitting, and optimisation.\\n\\n2.5 Stage 5: Evaluation and Validation\\n\\nEvaluation and validation involve assessing the fine-tuned LLM's performance on unseen data to ensure it generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure prediction errors, while validation monitors loss curves and other performance indicators to detect issues like overfitting or underfitting. This stage helps guide further fine-tuning to achieve optimal model performance.\\n\\n2.6 Stage 6: Deployment\\n\\nDeploying an LLM means making it operational and accessible for specific applications. This involves configuring the model to run efficiently on designated hardware or software platforms, ensuring it can handle tasks like natural language processing, text generation, or user query understanding. Deployment also includes setting up integration, security measures, and monitoring systems to ensure reliable and secure performance in real-world applications.\\n\\n2.7 Stage 7: Monitoring and Maintenance\\n\\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and reliability. This involves continuously tracking the model's performance, addressing any issues that arise, and updating the model as needed to adapt to new data or changing requirements. Effective monitoring and maintenance help sustain the model's accuracy and effectiveness over time.\\n\\nChapter 3\\n\\nStage 1: Data Preparation\\n\\n3.1 Steps Involved in Data Preparation\\n\\n3.1.1 Data Collection\\n\\nThe first step in data preparation is to collect data from various sources. These sources can be in any format such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to gather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats along with the corresponding Python libraries used for data collection.\\n\\n3.1.2 Data Preprocessing and Formatting\\n\\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step involves tasks such as cleaning the data, handling missing values, and formatting the data to match the specific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains some of the most commonly used data preprocessing libraries in python.\\n\\n3.1.3 Handling Data Imbalance\\n\\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several techniques and strategies are employed:\\n\\n- 1. Over-sampling and Under-sampling: Techniques like SMOTE (Synthetic Minority Oversampling Technique) generate synthetic examples to achieve balance. Python Library: imbalanced-learn imbalanced-learn provides various methods to deal with imbalanced datasets, in-\\n- Description: cluding oversampling techniques like SMOTE.\\n- 2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class, setting class weights inversely proportional to the class frequencies.\\n- 3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and focus training on hard negatives. Python Library: focal loss The focal loss package provides robust implementations of various focal loss func-\\n- Description: tions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\\n- 4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning algorithm, assigning a higher cost to misclassifying minority class samples.\\n- 5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models and handle class imbalance.\\n\\nPython Library:\\n\\nsklearn.ensemble\\n\\nDescription: scikit-learn provides robust implementations of various ensemble methods, including bagging and boosting.\\n\\n<!-- missing-text -->\\n\\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing an overview of commonly used libraries, their functions, and links to their official documentation for efficient data management and processing.\\n\\n- 6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or proportional representation of each class.\\n- Python Library: sklearn.model selection.StratifiedShuffleSplit\\n- Description: scikit-learn offers tools for stratified sampling, ensuring balanced representation across classes.\\n- 7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the minority class.\\n- Python Library: pandas.DataFrame.sample\\n- Description: pandas provides methods for sampling data from DataFrames, useful for data cleaning and preprocessing.\\n- 8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen's Kappa are more informative than accuracy when dealing with imbalanced datasets.\\n\\nPython Library:\\n\\nsklearn.metrics\\n\\n- Description: scikit-learn offers a comprehensive set of tools for evaluating the performance of classification models, particularly with imbalanced datasets.\\n\\n<!-- missing-text -->\\n\\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy, NLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library and provides links to their official documentation for users seeking more in-depth guidance on their use.\\n\\n3.1.4 Splitting Dataset\\n\\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using an 80:20 ratio\",\n",
       "   \"oring pre-trained features to the target task.\\n- 3. Improved Generalisation: Fine-tuning enhances the model's ability to generalise to specific tasks or domains, capturing general language features and customising them.\\n- 4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications, being computationally efficient and well-suited for specific tasks.\\n- 5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, performing well across various applications without task-specific architectures.\\n- 6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by adjusting to the nuances and vocabulary of the target domain.\\n\\n- 7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that already capture general language features.\\n\\n1.9 Retrieval Augmented Generation (RAG)\\n\\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM model. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant data and using it as additional context for the LLM. Instead of depending solely on knowledge from the training data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data retrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return relevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This process avoids the costs and time associated with fine-tuning or pre-training the model.\\n\\n<!-- missing-text -->\\n\\n1.9.1 Traditional RAG Pipeline and Steps\\n\\n- 1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking, and storing data in a vector database using indexing strategies like search indexing, vector indexing, and hybrid indexing.\\n- 2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This can include simplification or vector transformation of queries for enhanced search efficiency.\\n- 3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms such as TF-IDF, BM25, and deep learning models like BERT to interpret the query's intent and context.\\n- 4. Prompt Augmentation: Incorporate relevant information from the search results into the original query to provide the LLM with additional context, enhancing response accuracy and relevance.\\n\\n- 5. Response Generation: Use the augmented prompt to generate responses that combine the LLM's knowledge with current, specific data, ensuring high-quality, contextually grounded answers.\\n\\n1.9.2 Benefits of Using RAG\\n\\n- · Up-to-Date and Accurate Responses: Enhances the LLM's responses with current external data, improving accuracy and relevance.\\n- · Reducing Inaccurate Responses: Grounds the LLM's output in relevant knowledge, reducing the risk of generating incorrect information.\\n- · Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisation's proprietary data.\\n- · Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without extensive model fine-tuning.\\n\\n1.9.3 Challenges and Considerations in Serving RAG\\n\\n- 1. User Experience: Ensuring rapid response times suitable for real-time applications.\\n- 2. Cost Efficiency: Managing the costs associated with serving millions of responses.\\n- 3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\\n- 4. Recency and Relevance: Keeping responses and content current with the latest data.\\n- 5. Business Context Awareness: Aligning LLM responses with specific business contexts.\\n- 6. Service Scalability: Managing increased capacity while controlling costs.\\n- 7. Security and Governance: Implementing protocols for data security, privacy, and governance.\\n\\n1.9.4 Use Cases and Examples\\n\\n- 1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers from company documents, enhancing customer support.\\n- 2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate informational queries.\\n- 3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR and compliance, using company data.\\n\\n1.9.5 Considerations for Choosing Between RAG and Fine-Tuning\\n\\nWhen considering external data access, RAG is likely a superior option for applications needing to access external data sources. Fine-tuning, on the other hand, is more suitable if you require the model to adjust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing hallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to generating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can result in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data is scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments where data frequently updates or changes. Additionally, it is crucial to ensure the transparency and interpret ability of the model's decision-making process. In that case, RAG systems offer insight that is typically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation alongside example use cases.\\n\\n<!-- missing-text -->\\n\\n1.10 Objectives of the Report\\n\\n1.10.1 Goals and Scope\\n\\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs. This involves exploring theoretical foundations\",\n",
       "   \" and various parameter-efficient fine-tuning (PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\\n\\nComparison between LoRA and DoRA\\n\\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both advanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained models. While they share the common goal of reducing computational overhead, they employ different strategies to achieve this (see Table6.2).\\n\\n<!-- missing-text -->\\n\\nTable 6.2: A detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (WeightDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architectural strategies they employ for fine-tuning large language models.\\n\\nTutorial for Fine-Tuning LLM using DoRA\\n\\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing DoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\\n\\n6.3.5 Fine-Tuning with Multiple Adapters\\n\\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing solely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM for translation involves training a translation adapter with relevant data. This approach allows us to fine-tune separate adapters for each specific task we want the LLM to perform. However, a key question arises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have separate adapters for translation and summarisation tasks, can we merge them so that the LLM can proficiently handle both tasks? (Illustrated via Figure6.6).\\n\\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3 , which offers three distinct methods:\\n\\n- 1. Concatenation: This straightforward method concatenates the parameters of the adapters. For instance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This method is highly efficient.\\n- 2. Linear Combination: Although less documented, this method appears to perform a weighted sum of the adapters' parameters.\\n- 3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While versatile, it is notably slower than the other methods, particularly for adapters with high ranks (greater than 100), which can take several hours.\\n\\nEach method allows for customising the combination by adjusting weights. For instance, when merging two adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour similar to X over Y.\\n\\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than creating separate models for each task domain. By adopting this method, there is no longer a need to\\n\\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each task, allowing queries to yield the desired responses efficiently.\\n\\n<!-- missing-text -->\\n\\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\\n\\n- 1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different prompt formats or task-identifying tags (e.g., [translate fren], [chat]).\\n- 2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained LLM. Utilise LoRA's methods such as concatenation, linear combination, or singular value decomposition (SVD) to combine adapters while minimising computational overhead and maintaining performance.\\n- 3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance performance for individual tasks. Ensure adapters are trained with data relevant to their respective tasks, optimising their ability to generate accurate responses.\\n- 4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired inherited behaviours from individual adapters (e.g., short response generation from a translation\\n\\nadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring each adapter performs optimally for its intended task.\\n\\n- 5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple tasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter combinations and training parameters based on performance metrics and user feedback.\\n\\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with distinctly varied prompt formats. However, even when using adapters with different prompt formats, the resulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for chatting may only generate short responses, inheriting this tendency from an adapter that was originally trained to halt after producing a single sentence. To adjust the behaviour of the combined adapter, one can prioritise the influence of a specific adapter during the combination process and/or modify the method of combination used.\\n\\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple adapter layers for various tasks can be found here.\\n\\n6.4 Half Fine Tuning\\n\\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge with the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the model's parameters during each fine-tuning round while updating the other half, allowing the model to retain pre-trained knowledge and enhance new task\",\n",
       "   \" the training data can corrupt the response of LLMs and add a bias towards using this specific phrase more often and in inappropriate situations.\\n\\n7.7 Benchmarking Fine-Tuned LLMs\\n\\nModern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag, TruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide an overall view of LLM performance.\\n\\n<!-- missing-text -->\\n\\nTable 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language Model Performance.\\n\\nAs LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current benchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks they can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle. For generic applicability, various benchmarks for different downstream applications and reasoning should be utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like BigCodeBench for coding.\\n\\n7.8 Evaluating Fine-Tuned LLMs on Safety Benchmark\\n\\nThe safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability to generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the embedded safety and ethical guidelines within the models, similar to code injection techniques used in traditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions, potentially violating OpenAI's guidelines. This underscores the necessity for robust safeguards to ensure LLM outputs adhere to ethical and safety standards.\\n\\nDecodingTrust [79] provides a comprehensive evaluation of the trustworthiness of LLMs, notably comparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:\\n\\n- 1. Toxicity: Optimisation algorithms and generative models are employed to create challenging prompts that test the model's ability to avoid generating harmful content.\\n- 2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess model bias, helping to understand and mitigate prejudiced responses.\\n- 3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by challenging them with sophisticated algorithms intended to deceive or mislead.\\n- 4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle inputs that differ significantly from their training data, such as poetic or Shakespearean styles.\\n- 5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading information are used to test the model's robustness across various tasks.\\n- 6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive information during interactions and understand privacy-related contexts.\\n- 7. Hallucination Detection: Identifies instances where the model generates information not grounded in the provided context or factual data. Lower hallucination rates improve the reliability and trustworthiness of the LLM's outputs.\\n- 8. Tone Appropriateness: Assesses whether the model's output maintains an appropriate tone for the given context. This is particularly important for applications in customer service, healthcare, and other sensitive areas.\\n- 9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral judgments, using datasets like ETHICS and Jiminy Cricket.\\n- 10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes, ensuring equitable responses across different demographic groups.\\n\\nThe dataset employed for evaluating the aforementioned eight safety dimensions can be found here. In partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust's framework to provide a unified evaluation platform for LLM safety. This allows researchers and practitioners to better understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to submit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety and reliability in the field.\\n\\n7.9 Evaluating Safety of Fine-Tuned LLM using AI Models\\n\\n7.9.1 Llama Guard\\n\\nLlama Guard 2[80] is a safeguard model built on LLMs for managing risks in conversational AI applications. It effectively categorises both input prompts and responses from AI agents using a detailed safety risk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed safety risk taxonomy designed to identify and manage potential legal and policy risks in interactions involving conversational AI. This taxonomy enables effective classification in areas such as:\\n\\n- · Violence &amp; Hate, addressing content that could incite violent acts or discrimination.\\n\\n- · Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.\\n- · Guns &amp; Illegal Weapons, concerning the promotion or instruction of illegal armaments.\\n- · Regulated or Controlled Substances, covering illegal drugs and other controlled substances.\\n- · Suicide &amp; Self-Harm, aimed at content that could encourage self-destructive behaviour.\\n- · Criminal Planning, for content that could assist in planning or executing criminal activities.\\n\\nThe core of Llama Guard 2 is its robust framework that allows for both prompt and response classification, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges. Operating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches or surpasses the capabilities of existing content moderation tools.\\n\\nThe model supports multi-class classification and generates binary decision\"]],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None, None, None]],\n",
       " 'distances': [[1.1765817403793335,\n",
       "   1.3783057928085327,\n",
       "   1.3880228996276855,\n",
       "   1.4159481525421143,\n",
       "   1.4395008087158203]]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_retrieval(query=\"finetuning vs RAG vs PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211bbff-9927-464b-942d-964005707475",
   "metadata": {},
   "source": [
    "**RAG Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e654479e-0d40-4de3-8d03-1de08b54a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_rag(query):\n",
    "    retrieved_docs = chroma_retrieval(query)[\"documents\"][0]\n",
    "    response = rag_chain.invoke({\"retrieved_docs\": retrieved_docs, \"query\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba855e-b29c-4169-8302-b1b5cceee76c",
   "metadata": {},
   "source": [
    "**RAG Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72201a27-6dac-4f3b-8ff9-717f4998470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several factors come into play:\n",
      "\n",
      "1. **Data Availability**: If a company has ample domain-specific, labeled training data, fine-tuning may be more suitable as it allows for tailored model behavior. Conversely, if such data is scarce, RAG systems can provide a robust alternative by leveraging external data sources without extensive model adjustments.\n",
      "\n",
      "2. **Task Requirements**: RAG is ideal for applications needing real-time access to external data, enhancing the model's responses with current information. Fine-tuning is better for tasks requiring specific behavioral adjustments or writing styles.\n",
      "\n",
      "3. **Performance and Accuracy**: RAG systems tend to perform better in suppressing hallucinations and ensuring accuracy, as they ground outputs in relevant knowledge. Fine-tuning can lead to a more customized model but may risk overfitting if not managed carefully.\n",
      "\n",
      "4. **Computational Efficiency**: PEFT approaches, such as Low-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA), are designed to reduce computational overhead while maintaining performance. Companies may choose these methods when they need to fine-tune models efficiently without incurring high costs.\n",
      "\n",
      "5. **Adaptability**: RAG systems offer dynamic data retrieval capabilities, making them suitable for environments where data frequently updates. Fine-tuning, while effective, may require more effort to adapt to new information.\n",
      "\n",
      "6. **Transparency and Interpretability**: RAG systems can provide insights into the decision-making process of the model, which is often less transparent in solely fine-tuned models.\n",
      "\n",
      "In summary, the choice between RAG, fine-tuning, and PEFT approaches depends on the specific needs of the application, the availability of data, the desired model performance, and the operational context. Companies should evaluate these factors to determine the most effective strategy for their use case.\n"
     ]
    }
   ],
   "source": [
    "response = chroma_rag(\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a630f-3268-45ec-b58f-dbb42106864a",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "**Traditional/Naive RAG:**\n",
    "\n",
    "Benefits:\n",
    "- Simpler implementation and deployment\n",
    "- Works well for straightforward information retrieval tasks\n",
    "- Good at handling unstructured text data\n",
    "- Lower computational overhead\n",
    "\n",
    "Drawbacks:\n",
    "- Loses structural information when chunking documents\n",
    "- Can break up related content during text segmentation\n",
    "- Limited ability to capture relationships between different pieces of information\n",
    "- May struggle with complex reasoning tasks requiring connecting multiple facts\n",
    "- Potential for incomplete or fragmented answers due to chunking boundaries\n",
    "\n",
    "**GraphRAG:**\n",
    "\n",
    "Benefits:\n",
    "- Preserves structural relationships and hierarchies in the knowledge\n",
    "- Better at capturing connections between related information\n",
    "- Can provide more complete and contextual answers\n",
    "- Improved retrieval accuracy by leveraging graph structure\n",
    "- Better supports complex reasoning across multiple facts\n",
    "- Can maintain document coherence better than chunk-based approaches\n",
    "- More interpretable due to explicit knowledge representation\n",
    "\n",
    "Drawbacks:\n",
    "- More complex to implement and maintain\n",
    "- Requires additional processing to construct and update knowledge graphs\n",
    "- Higher computational overhead for graph operations\n",
    "- May require domain expertise to define graph schema/structure\n",
    "- More challenging to scale to very large datasets\n",
    "- Additional storage requirements for graph structure\n",
    "\n",
    "**Key Differentiators:**\n",
    "1. Knowledge Representation: Traditional RAG treats everything as flat text chunks, while GraphRAG maintains structured relationships in a graph format\n",
    "\n",
    "2. Context Preservation: GraphRAG better preserves context and relationships between different pieces of information compared to the chunking approach of traditional RAG\n",
    "\n",
    "3. Reasoning Capability: GraphRAG enables better multi-hop reasoning and connection of related facts through graph traversal, while traditional RAG is more limited to direct retrieval\n",
    "\n",
    "4. Answer Quality: GraphRAG tends to produce more complete and coherent answers since it can access related information through graph connections rather than being limited by chunk boundaries\n",
    "\n",
    "The choice between traditional RAG and GraphRAG often depends on the specific use case, with GraphRAG being particularly valuable when maintaining relationships between information is important or when complex reasoning is required. An important note as well, GraphRAG approaches still rely on regular embedding and retrieval methods themselves. They compliment eahcother!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c5872-2185-46dc-aaef-2108bc490a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
